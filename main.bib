@article{sinha2021,
  title = {{{UnNatural Language Inference}}},
  author = {Sinha, Koustuv and Parthasarathi, Prasanna and Pineau, Joelle and Williams, Adina},
  year = {2021},
  month = jun,
  journal = {arXiv:2101.00010 [cs]},
  eprint = {2101.00010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.00010},
  urldate = {2021-08-24},
  abstract = {Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random wordorder permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7\%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2021 - UnNatural Language Inference.pdf}
}
@article{sinha2019a,
  title = {{{CLUTRR}}: {{A Diagnostic Benchmark}} for {{Inductive Reasoning}} from {{Text}}},
  shorttitle = {{{CLUTRR}}},
  author = {Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L.},
  year = {2019},
  month = sep,
  journal = {arXiv:1908.06177 [cs, stat]},
  eprint = {1908.06177},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.06177},
  urldate = {2021-08-24},
  abstract = {The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs\textemdash with the graph-based model exhibiting both stronger generalization and greater robustness.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2019 - CLUTRR A Diagnostic Benchmark for Inductive Reaso.pdf}
}
@article{goodwin2020,
  title = {Probing {{Linguistic Systematicity}}},
  author = {Goodwin, Emily and Sinha, Koustuv and O'Donnell, Timothy J.},
  year = {2020},
  month = aug,
  journal = {arXiv:2005.04315 [cs]},
  eprint = {2005.04315},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.04315},
  urldate = {2021-08-24},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding models exhibit systematicity\textemdash generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models often generalize non-systematically. We examined the notion of systematicity from a linguistic perspective, defining a set of probes and a set of metrics to measure systematic behaviour. We also identified ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we performed a series of experiments in the setting of natural language inference (NLI), demonstrating that some NLU systems achieve high overall performance despite being non-systematic.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Goodwin et al. - 2020 - Probing Linguistic Systematicity.pdf}
}
@article{sinha2021a,
  title = {Masked {{Language Modeling}} and the {{Distributional Hypothesis}}: {{Order Word Matters Pre}}-Training for {{Little}}},
  shorttitle = {Masked {{Language Modeling}} and the {{Distributional Hypothesis}}},
  author = {Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau, Joelle and Williams, Adina and Kiela, Douwe},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.06644 [cs]},
  eprint = {2104.06644},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.06644},
  urldate = {2021-08-24},
  abstract = {A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks\textemdash including on tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pretraining, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2021 - Masked Language Modeling and the Distributional Hy.pdf}
}
@article{rogers2020,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2020},
  month = nov,
  journal = {arXiv:2002.12327 [cs]},
  eprint = {2002.12327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2020-11-11},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Adapter_BERT/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf}
}
@article{bahdanau2019,
  title = {Systematic {{Generalization}}: {{What Is Required}} and {{Can It Be Learned}}?},
  shorttitle = {Systematic {{Generalization}}},
  author = {Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and {de Vries}, Harm and Courville, Aaron},
  year = {2019},
  month = apr,
  journal = {arXiv:1811.12889 [cs]},
  eprint = {1811.12889},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.12889},
  urldate = {2019-12-08},
  abstract = {Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Zotero/storage/JMAQ8XCW/Bahdanau et al. - 2019 - Systematic Generalization What Is Required and Ca.pdf}
}
@article{gontier2020,
  title = {Measuring {{Systematic Generalization}} in {{Neural Proof Generation}} with {{Transformers}}},
  author = {Gontier, Nicolas and Sinha, Koustuv and Reddy, Siva and Pal, Christopher},
  year = {2020},
  month = oct,
  journal = {arXiv:2009.14786 [cs, stat]},
  eprint = {2009.14786},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.14786},
  urldate = {2021-08-24},
  abstract = {We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Gontier et al. - 2020 - Measuring Systematic Generalization in Neural Proo.pdf}
}
@article{sinha2020c,
  title = {Evaluating {{Logical Generalization}} in {{Graph Neural Networks}}},
  author = {Sinha, Koustuv and Sodhani, Shagun and Pineau, Joelle and Hamilton, William L.},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.06560 [cs, stat]},
  eprint = {2003.06560},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.06560},
  urldate = {2021-08-24},
  abstract = {Recent research has highlighted the role of relational inductive biases in building learning agents that can generalize and reason in a compositional manner. However, while relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand how effectively these approaches can adapt to new tasks. In this work, we study the task of logical generalization using GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. GraphLog consists of relation prediction tasks on 57 distinct logical domains. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task pretraining, and continual learning. Unlike previous benchmarks, our approach allows us to precisely control the logical relationship between the different tasks. We find that the ability for models to generalize and adapt is strongly determined by the diversity of the logical rules they encounter during training, and our results highlight new challenges for the design of GNN models. We publicly release the dataset and code used to generate and interact with the dataset at https://www.cs.mcgill.ca/ ksinha4/graphlog/.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2020 - Evaluating Logical Generalization in Graph Neural .pdf}
}
@article{sinha2020d,
  title = {Learning an {{Unreferenced Metric}} for {{Online Dialogue Evaluation}}},
  author = {Sinha, Koustuv and Parthasarathi, Prasanna and Wang, Jasmine and Lowe, Ryan and Hamilton, William L. and Pineau, Joelle},
  year = {2020},
  month = may,
  journal = {arXiv:2005.00583 [cs]},
  eprint = {2005.00583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.00583},
  urldate = {2021-08-24},
  abstract = {Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2020 - Learning an Unreferenced Metric for Online Dialogu.pdf}
}
@article{sachan2021,
  title = {Do {{Syntax Trees Help Pre}}-Trained {{Transformers Extract Information}}?},
  author = {Sachan, Devendra Singh and Zhang, Yuhao and Qi, Peng and Hamilton, William},
  year = {2021},
  month = jan,
  journal = {arXiv:2008.09084 [cs]},
  eprint = {2008.09084},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.09084},
  urldate = {2021-08-25},
  abstract = {Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pretrained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sachan et al. - 2021 - Do Syntax Trees Help Pre-trained Transformers Extr.pdf}
}
@article{fundel2007relex,
  title={RelEx—Relation extraction using dependency parse trees},
  author={Fundel, Katrin and K{\"u}ffner, Robert and Zimmer, Ralf},
  journal={Bioinformatics},
  volume={23},
  number={3},
  pages={365--371},
  year={2007},
  publisher={Oxford University Press}
}
@article{jie2019dependency,
  title={Dependency-guided LSTM-CRF for named entity recognition},
  author={Jie, Zhanming and Lu, Wei},
  journal={arXiv preprint arXiv:1909.10148},
  year={2019}
}
@article{strubell2018,
  title = {Linguistically-{{Informed Self}}-{{Attention}} for {{Semantic Role Labeling}}},
  author = {Strubell, Emma and Verga, Patrick and Andor, Daniel and Weiss, David and McCallum, Andrew},
  year = {2018},
  month = nov,
  journal = {arXiv:1804.08199 [cs]},
  eprint = {1804.08199},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1804.08199},
  urldate = {2021-08-25},
  abstract = {Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-ofspeech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on outof-domain data, nearly 10\% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Strubell et al. - 2018 - Linguistically-Informed Self-Attention for Semanti.pdf}
}
@article{stanojevic2021,
  title = {Formal {{Basis}} of a {{Language Universal}}},
  author = {Stanojevi{\'c}, Milo{\v s} and Steedman, Mark},
  year = {2021},
  month = apr,
  journal = {Computational Linguistics},
  volume = {47},
  number = {1},
  pages = {9--42},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00394},
  abstract = {Abstract             Steedman (2020) proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the ``separable'' permutations. This class of permutations is exactly the class that can be expressed in combinatory categorial grammars (CCGs). The excluded non-separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in word order in nominal and verbal constructions.             The number of permutations that are separable grows in the number n of lexical elements in the construction as the Large Schr\"oder Number Sn-1. Because that number grows much more slowly than the n! number of all permutations, this generalization is also of considerable practical interest for computational applications such as parsing and machine translation.             The present article examines the mathematical and computational origins of this restriction, and the reason it is exactly captured in CCG without the imposition of any further constraints.},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Stanojević and Steedman - 2021 - Formal Basis of a Language Universal.pdf}
}
@article{maudslay2020b,
  title = {It's {{All}} in the {{Name}}: {{Mitigating Gender Bias}} with {{Name}}-{{Based Counterfactual Data Substitution}}},
  shorttitle = {It's {{All}} in the {{Name}}},
  author = {Maudslay, Rowan Hall and Gonen, Hila and Cotterell, Ryan and Teufel, Simone},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.00871 [cs]},
  eprint = {1909.00871},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.00871},
  urldate = {2021-08-25},
  abstract = {This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19\% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49\%), thus improving on the state-of-the-art for bias mitigation.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Maudslay et al. - 2020 - It's All in the Name Mitigating Gender Bias with .pdf}
}
@article{gururangan2018a,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel R. and Smith, Noah A.},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.02324 [cs]},
  eprint = {1803.02324},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.02324},
  urldate = {2020-11-10},
  abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67\% of SNLI (Bowman et al., 2015) and 53\% of MultiNLI (Williams et al., 2018). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Gururangan et al. - 2018 - Annotation Artifacts in Natural Language Inference.pdf}
}
@article{nie2020,
  title = {Adversarial {{NLI}}: {{A New Benchmark}} for {{Natural Language Understanding}}},
  shorttitle = {Adversarial {{NLI}}},
  author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  year = {2020},
  month = may,
  journal = {arXiv:1910.14599 [cs]},
  eprint = {1910.14599},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.14599},
  urldate = {2021-08-25},
  abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-theart models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Nie et al. - 2020 - Adversarial NLI A New Benchmark for Natural Langu.pdf}
}
@article{mccoy2019,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  month = jun,
  journal = {arXiv:1902.01007 [cs]},
  eprint = {1902.01007},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.01007},
  urldate = {2020-11-10},
  abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf}
}
