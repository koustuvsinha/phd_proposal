@article{sinha2021,
  title = {{{UnNatural Language Inference}}},
  author = {Sinha, Koustuv and Parthasarathi, Prasanna and Pineau, Joelle and Williams, Adina},
  year = {2021},
  month = jun,
  journal = {arXiv:2101.00010 [cs]},
  eprint = {2101.00010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.00010},
  urldate = {2021-08-24},
  abstract = {Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random wordorder permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7\%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2021 - UnNatural Language Inference.pdf}
}
@article{sinha2019a,
  title = {{{CLUTRR}}: {{A Diagnostic Benchmark}} for {{Inductive Reasoning}} from {{Text}}},
  shorttitle = {{{CLUTRR}}},
  author = {Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L.},
  year = {2019},
  month = sep,
  journal = {arXiv:1908.06177 [cs, stat]},
  eprint = {1908.06177},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.06177},
  urldate = {2021-08-24},
  abstract = {The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs\textemdash with the graph-based model exhibiting both stronger generalization and greater robustness.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2019 - CLUTRR A Diagnostic Benchmark for Inductive Reaso.pdf}
}
@article{goodwin2020,
  title = {Probing {{Linguistic Systematicity}}},
  author = {Goodwin, Emily and Sinha, Koustuv and O'Donnell, Timothy J.},
  year = {2020},
  month = aug,
  journal = {arXiv:2005.04315 [cs]},
  eprint = {2005.04315},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.04315},
  urldate = {2021-08-24},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding models exhibit systematicity\textemdash generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models often generalize non-systematically. We examined the notion of systematicity from a linguistic perspective, defining a set of probes and a set of metrics to measure systematic behaviour. We also identified ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we performed a series of experiments in the setting of natural language inference (NLI), demonstrating that some NLU systems achieve high overall performance despite being non-systematic.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Goodwin et al. - 2020 - Probing Linguistic Systematicity.pdf}
}
@article{sinha2021a,
  title = {Masked {{Language Modeling}} and the {{Distributional Hypothesis}}: {{Order Word Matters Pre}}-Training for {{Little}}},
  shorttitle = {Masked {{Language Modeling}} and the {{Distributional Hypothesis}}},
  author = {Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau, Joelle and Williams, Adina and Kiela, Douwe},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.06644 [cs]},
  eprint = {2104.06644},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.06644},
  urldate = {2021-08-24},
  abstract = {A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks\textemdash including on tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pretraining, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2021 - Masked Language Modeling and the Distributional Hy.pdf}
}
@article{rogers2020,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2020},
  month = nov,
  journal = {arXiv:2002.12327 [cs]},
  eprint = {2002.12327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2020-11-11},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Adapter_BERT/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf}
}
@article{bahdanau2019,
  title = {Systematic {{Generalization}}: {{What Is Required}} and {{Can It Be Learned}}?},
  shorttitle = {Systematic {{Generalization}}},
  author = {Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and {de Vries}, Harm and Courville, Aaron},
  year = {2019},
  month = apr,
  journal = {arXiv:1811.12889 [cs]},
  eprint = {1811.12889},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.12889},
  urldate = {2019-12-08},
  abstract = {Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Zotero/storage/JMAQ8XCW/Bahdanau et al. - 2019 - Systematic Generalization What Is Required and Ca.pdf}
}
@article{gontier2020,
  title = {Measuring {{Systematic Generalization}} in {{Neural Proof Generation}} with {{Transformers}}},
  author = {Gontier, Nicolas and Sinha, Koustuv and Reddy, Siva and Pal, Christopher},
  year = {2020},
  month = oct,
  journal = {arXiv:2009.14786 [cs, stat]},
  eprint = {2009.14786},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.14786},
  urldate = {2021-08-24},
  abstract = {We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Gontier et al. - 2020 - Measuring Systematic Generalization in Neural Proo.pdf}
}
@article{sinha2020c,
  title = {Evaluating {{Logical Generalization}} in {{Graph Neural Networks}}},
  author = {Sinha, Koustuv and Sodhani, Shagun and Pineau, Joelle and Hamilton, William L.},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.06560 [cs, stat]},
  eprint = {2003.06560},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.06560},
  urldate = {2021-08-24},
  abstract = {Recent research has highlighted the role of relational inductive biases in building learning agents that can generalize and reason in a compositional manner. However, while relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand how effectively these approaches can adapt to new tasks. In this work, we study the task of logical generalization using GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. GraphLog consists of relation prediction tasks on 57 distinct logical domains. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task pretraining, and continual learning. Unlike previous benchmarks, our approach allows us to precisely control the logical relationship between the different tasks. We find that the ability for models to generalize and adapt is strongly determined by the diversity of the logical rules they encounter during training, and our results highlight new challenges for the design of GNN models. We publicly release the dataset and code used to generate and interact with the dataset at https://www.cs.mcgill.ca/ ksinha4/graphlog/.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2020 - Evaluating Logical Generalization in Graph Neural .pdf}
}
@article{sinha2020d,
  title = {Learning an {{Unreferenced Metric}} for {{Online Dialogue Evaluation}}},
  author = {Sinha, Koustuv and Parthasarathi, Prasanna and Wang, Jasmine and Lowe, Ryan and Hamilton, William L. and Pineau, Joelle},
  year = {2020},
  month = may,
  journal = {arXiv:2005.00583 [cs]},
  eprint = {2005.00583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.00583},
  urldate = {2021-08-24},
  abstract = {Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sinha et al. - 2020 - Learning an Unreferenced Metric for Online Dialogu.pdf}
}
@article{sachan2021,
  title = {Do {{Syntax Trees Help Pre}}-Trained {{Transformers Extract Information}}?},
  author = {Sachan, Devendra Singh and Zhang, Yuhao and Qi, Peng and Hamilton, William},
  year = {2021},
  month = jan,
  journal = {arXiv:2008.09084 [cs]},
  eprint = {2008.09084},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.09084},
  urldate = {2021-08-25},
  abstract = {Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pretrained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Sachan et al. - 2021 - Do Syntax Trees Help Pre-trained Transformers Extr.pdf}
}
@article{fundel2007relex,
  title={RelEx—Relation extraction using dependency parse trees},
  author={Fundel, Katrin and K{\"u}ffner, Robert and Zimmer, Ralf},
  journal={Bioinformatics},
  volume={23},
  number={3},
  pages={365--371},
  year={2007},
  publisher={Oxford University Press}
}
@article{jie2019dependency,
  title={Dependency-guided LSTM-CRF for named entity recognition},
  author={Jie, Zhanming and Lu, Wei},
  journal={arXiv preprint arXiv:1909.10148},
  year={2019}
}
@article{strubell2018,
  title = {Linguistically-{{Informed Self}}-{{Attention}} for {{Semantic Role Labeling}}},
  author = {Strubell, Emma and Verga, Patrick and Andor, Daniel and Weiss, David and McCallum, Andrew},
  year = {2018},
  month = nov,
  journal = {arXiv:1804.08199 [cs]},
  eprint = {1804.08199},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1804.08199},
  urldate = {2021-08-25},
  abstract = {Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-ofspeech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on outof-domain data, nearly 10\% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Strubell et al. - 2018 - Linguistically-Informed Self-Attention for Semanti.pdf}
}
@article{stanojevic2021,
  title = {Formal {{Basis}} of a {{Language Universal}}},
  author = {Stanojevi{\'c}, Milo{\v s} and Steedman, Mark},
  year = {2021},
  month = apr,
  journal = {Computational Linguistics},
  volume = {47},
  number = {1},
  pages = {9--42},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00394},
  abstract = {Abstract             Steedman (2020) proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the ``separable'' permutations. This class of permutations is exactly the class that can be expressed in combinatory categorial grammars (CCGs). The excluded non-separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in word order in nominal and verbal constructions.             The number of permutations that are separable grows in the number n of lexical elements in the construction as the Large Schr\"oder Number Sn-1. Because that number grows much more slowly than the n! number of all permutations, this generalization is also of considerable practical interest for computational applications such as parsing and machine translation.             The present article examines the mathematical and computational origins of this restriction, and the reason it is exactly captured in CCG without the imposition of any further constraints.},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Stanojević and Steedman - 2021 - Formal Basis of a Language Universal.pdf}
}
@article{maudslay2020b,
  title = {It's {{All}} in the {{Name}}: {{Mitigating Gender Bias}} with {{Name}}-{{Based Counterfactual Data Substitution}}},
  shorttitle = {It's {{All}} in the {{Name}}},
  author = {Maudslay, Rowan Hall and Gonen, Hila and Cotterell, Ryan and Teufel, Simone},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.00871 [cs]},
  eprint = {1909.00871},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.00871},
  urldate = {2021-08-25},
  abstract = {This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19\% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49\%), thus improving on the state-of-the-art for bias mitigation.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Maudslay et al. - 2020 - It's All in the Name Mitigating Gender Bias with .pdf}
}
@article{gururangan2018a,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel R. and Smith, Noah A.},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.02324 [cs]},
  eprint = {1803.02324},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.02324},
  urldate = {2020-11-10},
  abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67\% of SNLI (Bowman et al., 2015) and 53\% of MultiNLI (Williams et al., 2018). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Gururangan et al. - 2018 - Annotation Artifacts in Natural Language Inference.pdf}
}
@article{nie2020,
  title = {Adversarial {{NLI}}: {{A New Benchmark}} for {{Natural Language Understanding}}},
  shorttitle = {Adversarial {{NLI}}},
  author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  year = {2020},
  month = may,
  journal = {arXiv:1910.14599 [cs]},
  eprint = {1910.14599},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.14599},
  urldate = {2021-08-25},
  abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-theart models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Nie et al. - 2020 - Adversarial NLI A New Benchmark for Natural Langu.pdf}
}
@article{mccoy2019,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  month = jun,
  journal = {arXiv:1902.01007 [cs]},
  eprint = {1902.01007},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.01007},
  urldate = {2020-11-10},
  abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf}
}
@article{parthasarathi2021a,
  title = {Sometimes {{We Want Translationese}}},
  author = {Parthasarathi, Prasanna and Sinha, Koustuv and Pineau, Joelle and Williams, Adina},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.07623 [cs]},
  eprint = {2104.07623},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.07623},
  urldate = {2021-08-26},
  abstract = {Rapid progress in Neural Machine Translation (NMT) systems over the last few years has been driven primarily towards improving translation quality, and as a secondary focus, improved robustness to input perturbations (e.g. spelling and grammatical mistakes). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness and faithfulness, focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure the effects on the target side in terms of both robustness and faithfulness. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Parthasarathi et al. - 2021 - Sometimes We Want Translationese2.pdf}
}
@inproceedings{hinton1986learning,
  title={Learning distributed representations of concepts},
  author={Hinton, Geoffrey E and others},
  booktitle={Proceedings of the eighth annual conference of the cognitive science society},
  volume={1},
  pages={12},
  year={1986},
  organization={Amherst, MA}
}
@article{muggleton1991inductive,
  title={Inductive logic programming},
  author={Muggleton, Stephen},
  journal={New generation computing},
  volume={8},
  number={4},
  pages={295--318},
  year={1991},
  publisher={Springer}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  booktitle={Advances in neural information processing systems},
  pages={4967--4976},
  year={2017}
}

@inproceedings{
    hudson2018compositional,
    title={Compositional Attention Networks for Machine Reasoning},
    author={Drew Arad Hudson and Christopher D. Manning},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=S1Euwz-Rb},
}

@inproceedings{santoro2018relational,
  title={Relational recurrent neural networks},
  author={Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7310--7321},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{
    Velickovic2017-mh,
    title={Graph Attention Networks},
    author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=rJXMpikCZ},
}

@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1724--1734},
  year={2014}
}

@inproceedings{lake2017generalization,
  title={Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International Conference on Machine Learning},
  pages={2879--2888},
  year={2018}
}

@inproceedings{
    bahdanau2018systematic,
    title={Systematic Generalization: What Is Required and Can It Be Learned?},
    author={Dzmitry Bahdanau and Shikhar Murty and Michael Noukhovitch and Thien Huu Nguyen and Harm de Vries and Aaron Courville},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=HkezXnA9YX},
}

@ARTICLE{2018arXiv181107017S,
   author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua.},
    title = "{On Training Recurrent Neural Networks for Lifelong Learning}",
  journal = {arXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1811.07017},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
     year = 2018,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181107017S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@ARTICLE{Evans2017-pu,
  title         = "Learning Explanatory Rules from Noisy Data",
  author        = "Evans, Richard and Grefenstette, Edward",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  eprint        = "1711.04574",
  primaryClass  = "cs.NE",
  arxivid       = "1711.04574"
}

@book{APA:83,
  author          = {{American Psychological Association}},
  title           = {Publications Manual},
  year            = 1983,
  publisher       = {American Psychological Association}
}

@book{Aho:72,
  author          = {Alfred V. Aho and Jeffrey D. Ullman},
  title           = {The Theory of Parsing, Translation and Compiling},
  year            = 1972,
  publisher       = {Prentice-Hall}
}

@article{Ando2005,
  Author          = {Ando, Rie Kubota and Zhang, Tong},
  Journal         = {Journal of Machine Learning Research},
  Title           = {A Framework for Learning Predictive Structures from
                  Multiple Tasks and Unlabeled Data},
  Year            = 2005
}

@article{Chandra:81,
  author          = {Ashok K. Chandra and Dexter C. Kozen and Larry J.
                  Stockmeyer},
  year            = 1981,
  title           = {Alternation},
  journal         = {Journal of the Association for Computing Machinery}
}

@book{Gusfield:97,
  author          = {Dan Gusfield},
  title           = {Algorithms on Strings, Trees and Sequences},
  year            = 1997,
  publisher       = {Cambridge University Press}
}

@article{Li-etal-2020-SANLI,
  author          = {Peiguang Li and Hongfeng Yu and Wenkai Zhang and Guangluan
                  Xu and Xian Sun},
  title           = {{SA-NLI:} {A} Supervised Attention based framework for
                  Natural Language Inference},
  journal         = {Neurocomputing},
  year            = 2020,
  url = {https://doi.org/10.1016/j.neucom.2020.03.092}
}

@inproceedings{abeille-1990-lexical,
  title           = "Lexical and Syntactic Rules in a {T}ree {A}djoining
                  {G}rammar",
  author          = "Abeille, Anne",
  booktitle       = "28th Annual Meeting of the Association for Computational
                  Linguistics",
  month           = jun,
  year            = 1990,
  address         = "Pittsburgh, Pennsylvania, USA",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P90-1037",
  doi             = "10.3115/981823.981860",
  pages           = "292--298",
}

@inproceedings{an-etal-2019-representation,
  title           = "Representation of Constituents in Neural Language Models:
                  Coordination Phrase as a Case Study",
  author          = "An, Aixiu and Qian, Peng and Wilcox, Ethan and Levy, Roger",
  booktitle       = "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month           = nov,
  year            = 2019,
  address         = "Hong Kong, China",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D19-1287",
  doi             = "10.18653/v1/D19-1287",
  pages           = "2888--2899",
}

@inproceedings{andrew2007scalable,
  title           = {Scalable training of {L1}-regularized log-linear models},
  author          = {Andrew, Galen and Gao, Jianfeng},
  booktitle       = {Proceedings of the 24th International Conference on Machine
                  Learning},
  year            = 2007
}

@article{baddeley-etal-2009-working,
  title           = {Working memory and binding in sentence recall},
  author          = {Baddeley, Alan D and Hitch, Graham J and Allen, Richard J},
  journal         = {Journal of Memory and Language},
  year            = 2009,
  url = {https://doi.org/10.1016/j.jml.2009.05.004}
}

@article{bemis-pylkkanen-2013-basic,
  title           = {Basic linguistic composition recruits the left anterior
                  temporal lobe and left angular gyrus during both listening and
                  reading},
  author          = {Bemis, Douglas K and Pylkk{\"a}nen, Liina},
  journal         = {Cerebral Cortex},
  year            = 2013
}

@inproceedings{bernardy-lappin-2017-using,
  title           = "Using Deep Neural Networks to Learn Syntactic Agreement",
  author          = "Bernardy, Jean-Phillipe and Lappin, Shalom",
  booktitle       = "Linguistic Issues in Language Technology, Volume 15, 2017",
  year            = 2017,
  publisher       = "CSLI Publications",
  url             = "https://www.aclweb.org/anthology/2017.lilt-15.3",
}

@article{blei-etal-2003-latent,
  title           = {Latent dirichlet allocation},
  author          = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  journal         = {JMLR},
  year            = 2003,
  url = {https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf}
}

@techreport{wolf2019some,
  title={Some additional experiments extending the tech report” Assessing BERTs syntactic abilities” by Yoav Goldberg},
  author={Wolf, Thomas},
  year={2019},
  institution={HuggingFace},
  url={https://huggingface.co/bert-syntax/extending-bert-syntax.pdf}
}

@inproceedings{bowman-etal-2015-large,
  title           = "A large annotated corpus for learning natural language
                  inference",
  author          = "Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher
                  and Manning, Christopher D.",
  booktitle       = "Proceedings of the 2015 Conference on Empirical Methods in
                  Natural Language Processing",
  month           = sep,
  year            = 2015,
  address         = "Lisbon, Portugal",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D15-1075",
  doi             = "10.18653/v1/D15-1075",
  pages           = "632--642",
}

@book{bresnan-etal-2015-lexical,
  title           = {Lexical-functional syntax},
  author          = {Bresnan, Joan and Asudeh, Ash and Toivonen, Ida and
                  Wechsler, Stephen},
  year            = 2015,
  publisher       = {John Wiley \& Sons}
}

@inproceedings{brown-etal-2020-gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{cattell-1886-time,
    author = {Cattell, James McKeen},
    title = {The Time it takes to see and name objects},
    journal = {Mind},
    volume = {os-XI},
    number = {41},
    pages = {63-65},
    year = {1886},
    month = {01},
    issn = {0026-4423},
    doi = {10.1093/mind/os-XI.41.63},
    url = {https://doi.org/10.1093/mind/os-XI.41.63},
    eprint = {https://academic.oup.com/mind/article-pdf/os-XI/41/63/9358440/os-XI\_41\_63.pdf},
}


@inproceedings{chaves-2020-dont,
  title           = "What Don{'}t {RNN} Language Models Learn About Filler-Gap
                  Dependencies?",
  author          = "Chaves, Rui",
  booktitle       = "Proceedings of the Society for Computation in Linguistics
                  2020",
  month           = jan,
  year            = 2020,
  address         = "New York, New York",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.scil-1.1",
  pages           = "1--11",
}

@inproceedings{chen-etal-2017-enhanced,
  title           = "Enhanced {LSTM} for Natural Language Inference",
  author          = "Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Wei, Si
                  and Jiang, Hui and Inkpen, Diana",
  booktitle       = "Proceedings of the 55th Annual Meeting of the Association
                  for Computational Linguistics (Volume 1: Long Papers)",
  month           = jul,
  year            = 2017,
  address         = "Vancouver, Canada",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P17-1152",
  doi             = "10.18653/v1/P17-1152",
  pages           = "1657--1668",
}

@book{chierchia-mcconnell-1990-meaning,
  title           = {Meaning and grammar: An Introduction to Semantics},
  author          = {Chierchia, Gennaro and McConnell-Ginet, Sally},
  year            = 1990,
  publisher       = {Cambridge, Ma: MIT Press}
}

@book{chomsky-1995-minimalist,
  title           = {The minimalist program},
  author          = {Chomsky, Noam},
  year            = 1995,
  publisher       = {Cambridge, Massachusetts: The MIT Press}
}

@book{chomsky-1957-syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  year={1957},
  publisher={Walter de Gruyter}
}

@inproceedings{chrupala-alishahi-2019-correlating,
  title           = "Correlating Neural and Symbolic Representations of
                  Language",
  author          = "Chrupa{\l}a, Grzegorz and Alishahi, Afra",
  booktitle       = "Proceedings of the 57th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P19-1283",
  doi             = "10.18653/v1/P19-1283",
  pages           = "2952--2962",
}

@inproceedings{clark-etal-2019-bert,
  title           = "What Does {BERT} Look at? An Analysis of {BERT}{'}s
                  Attention",
  author          = "Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and
                  Manning, Christopher D.",
  booktitle       = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
                  and Interpreting Neural Networks for NLP",
  month           = aug,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/W19-4828",
  doi             = "10.18653/v1/W19-4828",
  pages           = "276--286",
}

@inproceedings{condoravdi-etal-2003-entailment,
  title           = "Entailment, intensionality and text understanding",
  author          = "Condoravdi, Cleo and Crouch, Dick and de Paiva, Valeria and
                  Stolle, Reinhard and Bobrow, Daniel G.",
  booktitle       = "Proceedings of the {HLT}-{NAACL} 2003 Workshop on Text
                  Meaning",
  year            = 2003,
  url             = "https://www.aclweb.org/anthology/W03-0906",
  pages           = "38--45",
}

@inproceedings{conneau-etal-2017-supervised,
    title = "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    author = {Conneau, Alexis  and
      Kiela, Douwe  and
      Schwenk, Holger  and
      Barrault, Lo{\"\i}c  and
      Bordes, Antoine},
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1070",
    doi = "10.18653/v1/D17-1070",
    pages = "670--680"
}

@inproceedings{da-costa-chaves-2020-assessing,
  title           = "Assessing the ability of Transformer-based Neural Models to
                  represent structurally unbounded dependencies",
  author          = "Da Costa, Jillian and Chaves, Rui",
  booktitle       = "Proceedings of the Society for Computation in Linguistics
                  2020",
  month           = jan,
  year            = 2020,
  address         = "New York, New York",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.scil-1.2",
  pages           = "12--21",
}

@inproceedings{dagan-etal-2005-pascal,
  title           = {The PASCAL recognising textual entailment challenge},
  author          = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle       = {Machine Learning Challenges Workshop},
  year            = 2005,
  organization    = {Springer},
  url = {https://dl.acm.org/doi/10.1007/11736790_9}
}

@incollection{dagan-etal-2006,
  Author          = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  Booktitle       = {Machine learning challenges. evaluating predictive
                  uncertainty, visual object classification, and recognising
                  tectual entailment},
  Publisher       = {Springer},
  Title           = {The {PASCAL} recognising textual entailment challenge},
  Year            = 2006
}

@inproceedings{dasgupta-etal-2018-evaluating,
  title           = {Evaluating compositionality in sentence embeddings},
  author          = {Dasgupta, Ishita and Guo, Demi and Stuhlm{\"u}ller, Andreas
                  and Gershman, Samuel J and Goodman, Noah D},
  booktitle       = {Proceedings of Annual Meeting of the Cognitive Science
                  Society},
  year            = 2018,
  url = {https://arxiv.org/abs/1802.04302}
}

@inproceedings{davis-van-schijndel-2020-recurrent,
  title           = "Recurrent Neural Network Language Models Always Learn
                  {E}nglish-Like Relative Clause Attachment",
  author          = "Davis, Forrest and van Schijndel, Marten",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-main.179",
  doi             = "10.18653/v1/2020.acl-main.179",
  pages           = "1979--1990",
}

@article{ettinger-2020-whatbertisnot,
    title = "What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
    author = "Ettinger, Allyson",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.3",
    doi = "10.1162/tacl_a_00298",
    pages = "34--48",
    abstract = "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction{---} and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
}


@article{frege1948sense,
  title           = {Sense and reference},
  author          = {Frege, Gottlob},
  journal         = {The philosophical review},
  year            = 1948
}

@inproceedings{gandhi2019mutual,
  author = {Gandhi, Kanishk and Lake, Brenden M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {14182--14192},
 publisher = {Curran Associates, Inc.},
 title = {Mutual exclusivity as a challenge for deep neural networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/a378383b89e6719e15cd1aa45478627c-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{gauthier-etal-2020-syntaxgym,
  title           = "{S}yntax{G}ym: An Online Platform for Targeted Evaluation
                  of Language Models",
  author          = "Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian,
                  Peng and Levy, Roger",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics: System Demonstrations",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-demos.10",
  doi             = "10.18653/v1/2020.acl-demos.10",
  pages           = "70--76",
}

@article{goldberg2019assessing,
  title           = {Assessing {BERT's} syntactic abilities},
  author          = {Goldberg, Yoav},
  journal         = {arXiv preprint arXiv:1901.05287},
  year            = 2019,
  url = {https://arxiv.org/abs/1901.05287}
}

@article{goodwin2020probing,
  Author          = {Emily Goodwin and Koustuv Sinha and Timothy J. O'Donnell},
  Title           = {Probing Linguistic Systematicity},
  Year            = 2020,
  journal         = {ACL}
}

@inproceedings{gulordava-etal-2018-colorless,
  title           = "Colorless Green Recurrent Networks Dream Hierarchically",
  author          = "Gulordava, Kristina and Bojanowski, Piotr and Grave,
                  Edouard and Linzen, Tal and Baroni, Marco",
  booktitle       = "Proceedings of the 2018 Conference of the North {A}merican
                  Chapter of the Association for Computational Linguistics:
                  Human Language Technologies, Volume 1 (Long Papers)",
  month           = jun,
  year            = 2018,
  address         = "New Orleans, Louisiana",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/N18-1108",
  doi             = "10.18653/v1/N18-1108",
  pages           = "1195--1205",
}

@inproceedings{gururangan-etal-2018-annotation,
  title           = "Annotation Artifacts in Natural Language Inference Data",
  author          = "Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer
                  and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.",
  booktitle       = "Proceedings of the 2018 Conference of the North {A}merican
                  Chapter of the Association for Computational Linguistics:
                  Human Language Technologies, Volume 2 (Short Papers)",
  month           = jun,
  year            = 2018,
  address         = "New Orleans, Louisiana",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/N18-2017",
  doi             = "10.18653/v1/N18-2017",
  pages           = "107--112",
}

@article{harris-1954-distributional,
  title           = {Distributional structure},
  author          = {Harris, Zellig S},
  journal         = {Word},
  year            = 1954
}

@inproceedings{hawkins-etal-2020-investigating,
  title           = "Investigating representations of verb bias in neural
                  language models",
  author          = "Hawkins, Robert and Yamakoshi, Takateru and Griffiths,
                  Thomas and Goldberg, Adele",
  booktitle       = "Proceedings of the 2020 Conference on Empirical Methods in
                  Natural Language Processing (EMNLP)",
  month           = nov,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.emnlp-main.376",
  doi             = "10.18653/v1/2020.emnlp-main.376",
  pages           = "4653--4663",
}

@book{heim-kratzer-1998-semantics,
  title           = {Semantics in generative grammar},
  author          = {Heim, Irene and Kratzer, Angelika},
  year            = 1998,
  publisher       = {Blackwell Oxford}
}

@inproceedings{hewitt-manning-2019-structural,
  title           = "{A} Structural Probe for Finding Syntax in Word
                  Representations",
  author          = "Hewitt, John and Manning, Christopher D.",
  booktitle       = "Proceedings of the 2019 Conference of the North {A}merican
                  Chapter of the Association for Computational Linguistics:
                  Human Language Technologies, Volume 1 (Long and Short Papers)",
  month           = jun,
  year            = 2019,
  address         = "Minneapolis, Minnesota",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/N19-1419",
  doi             = "10.18653/v1/N19-1419",
  pages           = "4129--4138",
}

@phdthesis{tabor-1994-syntactic,
  title={Syntactic innovation: A connectionist model},
  author={Tabor, Whitney},
  year={1994},
  university={Stanford University},
  url={https://www.proquest.com/openview/0a8f7e8a71e058b12053b545ca857fb2/1}
}

@inproceedings{hu-etal-2020-ocnli,
  title           = "{OCNLI}: {O}riginal {C}hinese {N}atural {L}anguage
                  {I}nference",
  author          = {Hu, Hai and Richardson, Kyle and Xu, Liang and Li, Lu and
                  K{\"u}bler, Sandra and Moss, Lawrence},
  booktitle       = "Findings of the Association for Computational Linguistics:
                  EMNLP 2020",
  month           = nov,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.findings-emnlp.314",
  doi             = "10.18653/v1/2020.findings-emnlp.314",
  pages           = "3512--3526",
}

@inproceedings{hu-etal-2020-systematic,
  title           = "A Systematic Assessment of Syntactic Generalization in
                  Neural Language Models",
  author          = "Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox,
                  Ethan and Levy, Roger",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-main.158",
  doi             = "10.18653/v1/2020.acl-main.158",
  pages           = "1725--1744",
}

@inproceedings{jawahar-etal-2019-bert,
  title           = "What Does {BERT} Learn about the Structure of Language?",
  author          = "Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah,
                  Djam{\'e}",
  booktitle       = "Proceedings of the 57th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P19-1356",
  doi             = "10.18653/v1/P19-1356",
  pages           = "3651--3657",
}

@inproceedings{jeretic-etal-2020-natural,
  title           = "Are Natural Language Inference Models {IMPPRESsive}?
                  {L}earning {IMPlicature} and {PRESupposition}",
  author          = "Jeretic, Paloma and Warstadt, Alex and Bhooshan, Suvrat and
                  Williams, Adina",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-main.768",
  doi             = "10.18653/v1/2020.acl-main.768",
  pages           = "8690--8705",
}

@article{kaplan-bresnan-1995-formal,
  title           = {Formal System for Grammatical Representation},
  author          = {Kaplan, Ronald M and Bresnan, Joan},
  journal         = {Formal Issues in Lexical-Functional Grammar},
  year            = 1995
}

@inproceedings{kodner-gupta-2020-overestimation,
  title           = "Overestimation of Syntactic Representation in Neural
                  Language Models",
  author          = "Kodner, Jordan and Gupta, Nitish",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-main.160",
  doi             = "10.18653/v1/2020.acl-main.160",
  pages           = "1757--1762",
}

@inproceedings{lewis-etal-2020-bart,
  title           = "{BART}: Denoising Sequence-to-Sequence Pre-training for
                  Natural Language Generation, Translation, and Comprehension",
  author          = "Lewis, Mike and Liu, Yinhan and Goyal, Naman and
                  Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer
                  and Stoyanov, Veselin and Zettlemoyer, Luke",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-main.703",
  doi             = "10.18653/v1/2020.acl-main.703",
  pages           = "7871--7880",
}

@inproceedings{lin-etal-2019-open,
  title           = "Open Sesame: Getting inside {BERT}{'}s Linguistic
                  Knowledge",
  author          = "Lin, Yongjie and Tan, Yi Chern and Frank, Robert",
  booktitle       = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
                  and Interpreting Neural Networks for NLP",
  month           = aug,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/W19-4825",
  doi             = "10.18653/v1/W19-4825",
  pages           = "241--253",
}

@article{linzen-baroni-2021-syntactic,
  title           = {Syntactic Structure from Deep Learning},
  author          = {Linzen, Tal and Baroni, Marco},
  journal         = {Annual Review of Linguistics},
  year            = 2021,
  url = {https://doi.org/10.1146/annurev-linguistics-032020-051035}
}

@article{linzen-etal-2016-assessing,
  title           = "Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive
                  Dependencies",
  author          = "Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav",
  journal         = "Transactions of the Association for Computational
                  Linguistics",
  volume          = 4,
  year            = 2016,
  url             = "https://www.aclweb.org/anthology/Q16-1037",
  doi             = "10.1162/tacl_a_00115",
  pages           = "521--535",
}

@inproceedings{linzen-leonard-2018-distinct,
  author          = {Tal Linzen and Brian Leonard},
  title           = {Distinct patterns of syntactic agreement errors in
                  recurrent networks and humans},
  booktitle       = {Proceedings of the 40th Annual Meeting of the Cognitive
                  Science Society},
  year            = 2018
}

@article{liu-et-al-2019-roberta,
  author          = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and
                  Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and
                  Luke Zettlemoyer and Veselin Stoyanov},
  title           = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining
                  Approach},
  year            = 2019,
  journal         = {arXiv},
  url = {https://arxiv.org/abs/1907.11692}
}

@article{manning-etal-2015-computational,
  title           = {Computational linguistics and deep learning},
  author          = {Manning, Christopher D},
  journal         = {Computational Linguistics},
  year            = 2015
}

@article {manning-etal-2020-emergent,
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	volume = {117},
	number = {48},
	pages = {30046--30054},
	year = {2020},
	doi = {10.1073/pnas.1907367117},
	publisher = {National Academy of Sciences},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.Code and most of the data to reproduce the analyses in this paper are freely available at https://github.com/clarkkev/attention-analysis and https://github.com/john-hewitt/structural-probes (55, 56); the BERT models are freely available at https://github.com/google-research/bert (20). The syntactic evaluations use the Penn Treebank-3 (PTB), which is available under license from he Linguistic Data consortium at https://catalog.ldc.upenn.edu/LDC99T42 (35).},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/48/30046},
	eprint = {https://www.pnas.org/content/117/48/30046.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@inproceedings{marvin-linzen-2018-targeted,
  title           = "Targeted Syntactic Evaluation of Language Models",
  author          = "Marvin, Rebecca and Linzen, Tal",
  booktitle       = "Proceedings of the 2018 Conference on Empirical Methods in
                  Natural Language Processing",
  month           = oct # "-" # nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D18-1151",
  doi             = "10.18653/v1/D18-1151",
  pages           = "1192--1202",
}

@inproceedings{mccoy-etal-2019-right,
  title           = "Right for the Wrong Reasons: Diagnosing Syntactic
                  Heuristics in Natural Language Inference",
  author          = "McCoy, Tom and Pavlick, Ellie and Linzen, Tal",
  booktitle       = "Proceedings of the 57th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P19-1334",
  doi             = "10.18653/v1/P19-1334",
  pages           = "3428--3448",
}

@inproceedings{mccoy-etal-2020-berts,
  title           = "{BERT}s of a feather do not generalize together: Large
                  variability in generalization across models with similar test
                  set performance",
  author          = "McCoy, R. Thomas and Min, Junghyun and Linzen, Tal",
  booktitle       = "Proceedings of the Third BlackboxNLP Workshop on Analyzing
                  and Interpreting Neural Networks for NLP",
  month           = nov,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.21",
  doi             = "10.18653/v1/2020.blackboxnlp-1.21",
  pages           = "217--227",
}

@inproceedings{mikolov2013efficient,
  author    = {Tom{\'{a}}s Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{montague-1970-universal,
  title           = {Universal grammar},
  author          = {Montague, Richard},
  journal         = {Theoria},
  year            = 1970
}

@inproceedings{naik-etal-2018-stress,
  title           = "Stress Test Evaluation for Natural Language Inference",
  author          = "Naik, Aakanksha and Ravichander, Abhilasha and Sadeh,
                  Norman and Rose, Carolyn and Neubig, Graham",
  booktitle       = "Proceedings of the 27th International Conference on
                  Computational Linguistics",
  month           = aug,
  year            = 2018,
  address         = "Santa Fe, New Mexico, USA",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/C18-1198",
  pages           = "2340--2353",
}

@inproceedings{naik-etal-2019-exploring,
  title           = "Exploring Numeracy in Word Embeddings",
  author          = "Naik, Aakanksha and Ravichander, Abhilasha and Rose,
                  Carolyn and Hovy, Eduard",
  booktitle       = "Proceedings of the 57th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P19-1329",
  doi             = "10.18653/v1/P19-1329",
  pages           = "3374--3380",
}

@inproceedings{nie-etal-2020-adversarial,
  title           = "Adversarial {NLI}: A New Benchmark for Natural Language
                  Understanding",
  author          = "Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal,
                  Mohit and Weston, Jason and Kiela, Douwe",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/2020.acl-main.441",
  doi             = "10.18653/v1/2020.acl-main.441",
  pages           = "4885--4901",
}

@inproceedings{ott2019fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53",
    abstract = "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
}


@inproceedings{poliak-etal-2018-hypothesis,
  title           = "Hypothesis Only Baselines in Natural Language Inference",
  author          = "Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita
                  and Rudinger, Rachel and Van Durme, Benjamin",
  booktitle       = "Proceedings of the Seventh Joint Conference on Lexical and
                  Computational Semantics",
  month           = jun,
  year            = 2018,
  address         = "New Orleans, Louisiana",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/S18-2023",
  doi             = "10.18653/v1/S18-2023",
  pages           = "180--191",
}

@book{pollard-sag-1994-head,
  title           = {Head-driven phrase structure grammar},
  author          = {Pollard, Carl and Sag, Ivan A},
  year            = 1994,
  publisher       = {University of Chicago Press},
  url = {https://web.stanford.edu/group/cslipublications/cslipublications/site/0226674479.shtml}
}

@inproceedings{prasad-etal-2019-using,
  title           = "Using Priming to Uncover the Organization of Syntactic
                  Representations in Neural Language Models",
  author          = "Prasad, Grusha and van Schijndel, Marten and Linzen, Tal",
  booktitle       = "Proceedings of the 23rd Conference on Computational Natural
                  Language Learning (CoNLL)",
  month           = nov,
  year            = 2019,
  address         = "Hong Kong, China",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/K19-1007",
  doi             = "10.18653/v1/K19-1007",
  pages           = "66--76",
}

@article{pylkkanen-etal-2014-building,
  title           = {Building phrases in language production: An MEG study of
                  simple composition},
  author          = {Pylkk{\"a}nen, Liina and Bemis, Douglas K and Elorrieta,
                  Estibaliz Blanco},
  journal         = {Cognition},
  year            = 2014
}

@article{radford-etal-2019-language,
  title           = {Language models are unsupervised multitask learners},
  author          = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan,
                  David and Amodei, Dario and Sutskever, Ilya},
  journal         = {OpenAI blog},
  year            = 2019,
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@article{rasooli-tetrault-2015,
  author          = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
  title           = {Yara Parser: {A} Fast and Accurate Dependency Parser},
  journal         = {Computing Research Repository},
  year            = 2015
}

@inproceedings{ravfogel-etal-2018-lstm,
  title           = "Can {LSTM} Learn to Capture Agreement? The Case of
                  {B}asque",
  author          = "Ravfogel, Shauli and Goldberg, Yoav and Tyers, Francis",
  booktitle       = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
                  Analyzing and Interpreting Neural Networks for {NLP}",
  month           = nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/W18-5412",
  doi             = "10.18653/v1/W18-5412",
  pages           = "98--107",
}

@inproceedings{ravfogel-etal-2019-studying,
  title           = "Studying the Inductive Biases of {RNN}s with Synthetic
                  Variations of Natural Languages",
  author          = "Ravfogel, Shauli and Goldberg, Yoav and Linzen, Tal",
  booktitle       = "Proceedings of the 2019 Conference of the North {A}merican
                  Chapter of the Association for Computational Linguistics:
                  Human Language Technologies, Volume 1 (Long and Short Papers)",
  month           = jun,
  year            = 2019,
  address         = "Minneapolis, Minnesota",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/N19-1356",
  doi             = "10.18653/v1/N19-1356",
  pages           = "3532--3542",
}

@inproceedings{ravichander-etal-2019-equate,
  title           = "{EQUATE}: A Benchmark Evaluation Framework for Quantitative
                  Reasoning in Natural Language Inference",
  author          = "Ravichander, Abhilasha and Naik, Aakanksha and Rose,
                  Carolyn and Hovy, Eduard",
  booktitle       = "Proceedings of the 23rd Conference on Computational Natural
                  Language Learning (CoNLL)",
  month           = nov,
  year            = 2019,
  address         = "Hong Kong, China",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/K19-1033",
  doi             = "10.18653/v1/K19-1033",
  pages           = "349--361",
}


@misc{sanh2020distilbert,
  title           = {DistilBERT, a distilled version of BERT: smaller, faster,
                  cheaper and lighter},
  author          = {Victor Sanh and Lysandre Debut and Julien Chaumond and
                  Thomas Wolf},
  year            = 2020,
  journal         = {5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
  url = {https://arxiv.org/abs/1910.01108}
}

@inproceedings{schabes-etal-1988-parsing,
  title           = "Parsing Strategies with {`}Lexicalized{'} Grammars:
                  Application to {T}ree {A}djoining {G}rammars",
  author          = "Schabes, Yves and Abeille, Anne and Joshi, Aravind K.",
  booktitle       = "{C}oling {B}udapest 1988 Volume 2: {I}nternational
                  {C}onference on {C}omputational {L}inguistics",
  year            = 1988,
  url             = "https://www.aclweb.org/anthology/C88-2121",
}

@article{scheerer1981early,
  title           = {Early German approaches to experimental reading research:
                  The contributions of Wilhelm Wundt and Ernst Meumann},
  author          = {Scheerer, Eckart},
  journal         = {Psychological Research},
  year            = 1981
}

@article{snell-grainger-2017-sentence,
  title           = {The sentence superiority effect revisited},
  author          = {Snell, Joshua and Grainger, Jonathan},
  journal         = {Cognition},
  year            = 2017
}

@software{spacy2,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@inproceedings{tenney-etal-2019-bert,
  title           = "{BERT} Rediscovers the Classical {NLP} Pipeline",
  author          = "Tenney, Ian and Das, Dipanjan and Pavlick, Ellie",
  booktitle       = "Proceedings of the 57th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2019,
  address         = "Florence, Italy",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/P19-1452",
  doi             = "10.18653/v1/P19-1452",
  pages           = "4593--4601",
}

@article{toyota-2001-changes,
  title           = {Changes in the constraints of semantic and syntactic
                  congruity on memory across three age groups},
  author          = {Toyota, Hiroshi},
  journal         = {Perceptual and Motor Skills},
  year            = 2001,
  url = {https://pubmed.ncbi.nlm.nih.gov/11453195/}
}

@inproceedings{tsuchiya-2018-performance,
  title           = "Performance Impact Caused by Hidden Bias of Training Data
                  for Recognizing Textual Entailment",
  author          = "Tsuchiya, Masatoshi",
  booktitle       = "Proceedings of the Eleventh International Conference on
                  Language Resources and Evaluation ({LREC} 2018)",
  month           = may,
  year            = 2018,
  address         = "Miyazaki, Japan",
  publisher       = "European Language Resources Association (ELRA)",
  url             = "https://www.aclweb.org/anthology/L18-1239",
}

@inproceedings{van-schijndel-etal-2019-quantity,
  title           = "Quantity doesn{'}t buy quality syntax with neural language
                  models",
  author          = "van Schijndel, Marten and Mueller, Aaron and Linzen, Tal",
  booktitle       = "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month           = nov,
  year            = 2019,
  address         = "Hong Kong, China",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D19-1592",
  doi             = "10.18653/v1/D19-1592",
  pages           = "5831--5837",
}

@inproceedings{van-schijndel-linzen-2018-neural,
  title           = "A Neural Model of Adaptation in Reading",
  author          = "van Schijndel, Marten and Linzen, Tal",
  booktitle       = "Proceedings of the 2018 Conference on Empirical Methods in
                  Natural Language Processing",
  month           = oct # "-" # nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D18-1499",
  doi             = "10.18653/v1/D18-1499",
  pages           = "4704--4710",
}

@inproceedings{vanderwende2005syntax,
  title           = {What syntax can contribute in the entailment task},
  author          = {Vanderwende, Lucy and Dolan, William B},
  booktitle       = {Machine Learning Challenges Workshop},
  year            = 2005,
  organization    = {Springer},
  url = {https://dl.acm.org/doi/10.1007/11736790_11}
}

@inproceedings{vaswani-etal-2017-attention,
   author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {Attention is All you Need},
     url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
     volume = {30},
     year = {2017}
}

@inproceedings{wallace-etal-2019-universal,
  title           = "Universal Adversarial Triggers for Attacking and Analyzing
                  {NLP}",
  author          = "Wallace, Eric and Feng, Shi and Kandpal, Nikhil and
                  Gardner, Matt and Singh, Sameer",
  booktitle       = "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month           = nov,
  year            = 2019,
  address         = "Hong Kong, China",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D19-1221",
  doi             = "10.18653/v1/D19-1221",
  pages           = "2153--2162",
}

@inproceedings{wang-etal-2018-glue,
  title           = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for
                  Natural Language Understanding",
  author          = "Wang, Alex and Singh, Amanpreet and Michael, Julian and
                  Hill, Felix and Levy, Omer and Bowman, Samuel",
  booktitle       = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
                  Analyzing and Interpreting Neural Networks for {NLP}",
  month           = nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/W18-5446",
  doi             = "10.18653/v1/W18-5446",
  pages           = "353--355",
}

@inproceedings{wang-etal-2019-superglue,
  author          = {Alex Wang and Yada Pruksachatkun and Nikita Nangia and
                  Amanpreet Singh and Julian Michael and Felix Hill and Omer
                  Levy and Samuel R. Bowman},
  editor          = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer
                  and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman
                  Garnett},
  title           = {SuperGLUE: {A} Stickier Benchmark for General-Purpose
                  Language Understanding Systems},
  booktitle       = {NeurIPS},
  year            = 2019,
  url={https://papers.nips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html}
}

@inproceedings{warstadt-etal-2019-investigating,
  title           = "Investigating {BERT}{'}s Knowledge of Language: Five
                  Analysis Methods with {NPI}s",
  author          = "Warstadt, Alex and Cao, Yu and Grosu, Ioana and Peng, Wei
                  and Blix, Hagen and Nie, Yining and Alsop, Anna and Bordia,
                  Shikha and Liu, Haokun and Parrish, Alicia and Wang, Sheng-Fu
                  and Phang, Jason and Mohananey, Anhad and Htut, Phu Mon and
                  Jeretic, Paloma and Bowman, Samuel R.",
  booktitle       = "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month           = nov,
  year            = 2019,
  address         = "Hong Kong, China",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D19-1286",
  doi             = "10.18653/v1/D19-1286",
  pages           = "2877--2887",
}

@article{warstadt-etal-2019-neural,
    title = "Neural Network Acceptability Judgments",
    author = "Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1040",
    doi = "10.1162/tacl_a_00290",
    pages = "625--641"
}



@article{tarski-1956-concept,
  title={The concept of truth in formalized languages},
  author={Tarski, Alfred},
  journal={Logic, semantics, metamathematics},
  volume={2},
  number={152-278},
  pages={7},
  year={1956},
  publisher={Oxford}
}

@article{tarski-1933-concept,
  title={The concept of truth in the languages of the deductive sciences},
  author={Tarski, Alfred},
  journal={Prace Towarzystwa Naukowego Warszawskiego, Wydzial III Nauk Matematyczno-Fizycznych},
  volume={34},
  number={13-172},
  pages={198},
  year={1933}
}

@book{wittgenstein-1922-tractatus,
title={Tractatus Logico-Philosophicus},
author={Wittgenstein, Ludwig},
year={1922},
publisher={Harcourt, Brace \& Company, Inc.},
place={New York}}

@article{wen-etal-2019-parallel,
  title           = {Parallel, cascaded, interactive processing of words during
                  sentence reading},
  author          = {Wen, Yun and Snell, Joshua and Grainger, Jonathan},
  journal         = {Cognition},
  year            = 2019
}

@inproceedings{white-etal-2018-lexicosyntactic,
  title           = "Lexicosyntactic Inference in Neural Models",
  author          = "White, Aaron Steven and Rudinger, Rachel and Rawlins, Kyle
                  and Van Durme, Benjamin",
  booktitle       = "Proceedings of the 2018 Conference on Empirical Methods in
                  Natural Language Processing",
  month           = oct # "-" # nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/D18-1501",
  doi             = "10.18653/v1/D18-1501",
  pages           = "4717--4724",
}

@inproceedings{wilcox-etal-2018-rnn,
  title           = "What do {RNN} Language Models Learn about Filler{--}Gap
                  Dependencies?",
  author          = "Wilcox, Ethan and Levy, Roger and Morita, Takashi and
                  Futrell, Richard",
  booktitle       = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
                  Analyzing and Interpreting Neural Networks for {NLP}",
  month           = nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/W18-5423",
  doi             = "10.18653/v1/W18-5423",
  pages           = "211--221",
}

@inproceedings{williams-etal-2018-broad,
  title           = "A Broad-Coverage Challenge Corpus for Sentence
                  Understanding through Inference",
  author          = "Williams, Adina and Nangia, Nikita and Bowman, Samuel",
  booktitle       = "Proceedings of the 2018 Conference of the North {A}merican
                  Chapter of the Association for Computational Linguistics:
                  Human Language Technologies, Volume 1 (Long Papers)",
  month           = jun,
  year            = 2018,
  address         = "New Orleans, Louisiana",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/N18-1101",
  doi             = "10.18653/v1/N18-1101",
  pages           = "1112--1122",
}

@article{williams-etal-2018-latent,
    title = "Do latent tree learning models identify meaningful structure in sentences?",
    author = "Williams, Adina  and
      Drozdov, Andrew  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://www.aclweb.org/anthology/Q18-1019",
    doi = "10.1162/tacl_a_00019",
    pages = "253--267",
    abstract = "Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",
}

@inproceedings{wolf2020transformers,
  title           = {Transformers: State-of-the-art natural language processing},
  author          = {Wolf, Thomas and Chaumond, Julien and Debut, Lysandre and
                  Sanh, Victor and Delangue, Clement and Moi, Anthony and
                  Cistac, Pierric and Funtowicz, Morgan and Davison, Joe and
                  Shleifer, Sam and others},
  booktitle       = {EMNLP: System Demonstrations},
  year            = 2020
}

@article{yoon-etal-2018-dynamic,
  author          = {Deunsol Yoon and Dongbok Lee and SangKeun Lee},
  title           = {Dynamic Self-Attention : Computing Attention over Words
                  Dynamically for Sentence Embedding},
  journal         = {arXiv},
  year            = 2018
}

@inproceedings{zhang-bowman-2018-language,
  title           = "Language Modeling Teaches You More than Translation Does:
                  Lessons Learned Through Auxiliary Syntactic Task Analysis",
  author          = "Zhang, Kelly and Bowman, Samuel",
  booktitle       = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
                  Analyzing and Interpreting Neural Networks for {NLP}",
  month           = nov,
  year            = 2018,
  address         = "Brussels, Belgium",
  publisher       = "Association for Computational Linguistics",
  url             = "https://www.aclweb.org/anthology/W18-5448",
  doi             = "10.18653/v1/W18-5448",
  pages           = "359--361",
}

@inproceedings{zhao2015self,
  title={Self-adaptive hierarchical sentence model},
  author={Zhao, Han and Lu, Zhengdong and Poupart, Pascal},
  booktitle={Proceedings of the 24th International Conference on Artificial Intelligence},
  pages={4069--4076},
  year={2015},
  url={https://arxiv.org/abs/1504.05070}
}


@article{warstadt-etal-2020-blimp,
    title = "{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish",
    author = "Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.25",
    doi = "10.1162/tacl_a_00321",
    pages = "377--392",
    abstract = "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
}

@inproceedings{nangia-bowman-2019-human,
    title = "Human vs. Muppet: A Conservative Estimate of Human Performance on the {GLUE} Benchmark",
    author = "Nangia, Nikita  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1449",
    doi = "10.18653/v1/P19-1449",
    pages = "4566--4575",
    abstract = "The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@article{pham-etal-2020-out,
  title={Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?},
  author={Pham, Thang M and Bui, Trung and Mai, Long and Nguyen, Anh},
  journal={arXiv preprint arXiv:2012.15180},
  year={2020},
  url={https://arxiv.org/abs/2012.15180}
}

@article{gupta-etal-2021-bert,
  title={{BERT} \& Family Eat Word Salad: Experiments with Text Understanding},
  url={https://arxiv.org/abs/2101.03453},
  author={Gupta, Ashim and Kvernadze, Giorgi and Srikumar, Vivek},
  journal={AAAI},
  year={2021}
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.463",
    doi = "10.18653/v1/2020.acl-main.463",
    pages = "5185--5198",
    abstract = "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",
}

@inproceedings{wu-etal-2020-perturbed,
    title = "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting {BERT}",
    author = "Wu, Zhiyong  and
      Chen, Yun  and
      Kao, Ben  and
      Liu, Qun",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.383",
    doi = "10.18653/v1/2020.acl-main.383",
    pages = "4166--4176",
    abstract = "By introducing a small set of additional parameters, a \textit{probe} learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such \textit{probing} tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.",
}

@inproceedings{liu-etal-2019-linguistic,
    title = "Linguistic Knowledge and Transferability of Contextual Representations",
    author = "Liu, Nelson F.  and
      Gardner, Matt  and
      Belinkov, Yonatan  and
      Peters, Matthew E.  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1112",
    doi = "10.18653/v1/N19-1112",
    pages = "1073--1094",
    abstract = "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
}

@inproceedings{warstadt-bowman-2020-can,
  title={Can neural networks acquire a structural bias from raw linguistic data?},
  author={Warstadt, Alex and Bowman, Samuel R},
  booktitle={Proceedings of the 42nd Annual Virtual Meeting of the Cognitive Science Society},
  year={2020},
  url={https://arxiv.org/abs/2007.06761}
}

@article{mollica-2020-composition,
  title={Composition is the core driver of the language-selective network},
  author={Mollica, Francis and Siegelman, Matthew and Diachek, Evgeniia and Piantadosi, Steven T and Mineroff, Zachary and Futrell, Richard and Kean, Hope and Qian, Peng and Fedorenko, Evelina},
  journal={Neurobiology of Language},
  volume={1},
  number={1},
  pages={104--134},
  year={2020},
  publisher={MIT Press},
  url={https://direct.mit.edu/nol/article/1/1/104/10024/Composition-is-the-Core-Driver-of-the-Language}
}

@article{mirault2018you,
  title={You that read wrong again! A transposed-word effect in grammaticality judgments},
  author={Mirault, Jonathan and Snell, Joshua and Grainger, Jonathan},
  journal={Psychological Science},
  volume={29},
  number={12},
  pages={1922--1929},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{snell2019word,
  title={Word position coding in reading is noisy},
  author={Snell, Joshua and Grainger, Jonathan},
  journal={Psychonomic bulletin \& review},
  volume={26},
  number={2},
  pages={609--615},
  year={2019},
  publisher={Springer}
}

@article{sinha2021masked,
  title={Masked language modeling and the distributional hypothesis: Order word matters pre-training for little},
  author={Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau, Joelle and Williams, Adina and Kiela, Douwe},
  journal={arXiv preprint arXiv:2104.06644},
  year={2021},
  url={https://arxiv.org/abs/2104.06644}
}

@article{parthasarathi2021sometimes,
  title={Sometimes We Want Translationese},
  author={Parthasarathi, Prasanna and Sinha, Koustuv and Pineau, Joelle and Williams, Adina},
  journal={arXiv preprint arXiv:2104.07623},
  year={2021},
  url={https://arxiv.org/abs/2104.07623}
}

@article{alleman2021syntactic,
  title={Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models},
  author={Alleman, Matteo and Mamou, Jonathan and Del Rio, Miguel A and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  journal={arXiv preprint arXiv:2104.07578},
  year={2021},
  url={https://arxiv.org/abs/2104.07578}
}

@article{desai2020calibration,
      title={Calibration of Pre-trained Transformers},
      author={Shrey Desai and Greg Durrett},
      year={2020},
      eprint={2003.07892},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={EMNLP}
}


@inproceedings{kaya2019shallowdeep,
  Title = {{Shallow-Deep Networks}: Understanding and Mitigating Network Overthinking},
  Author = {Yigitcan Kaya and Sanghyun Hong and Tudor Dumitras},
  Booktitle = {Proceedings of the 2019 International Conference on Machine Learning (ICML)},
  Address = {Long Beach, CA},
  Month = {Jun},
  Year = {2019},
  url={https://arxiv.org/abs/1810.07052}
}

@inproceedings{zhou2020bert,
 author = {Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {18330--18341},
 publisher = {Curran Associates, Inc.},
 title = {BERT Loses Patience: Fast and Robust Inference with Early Exit},
 url = {https://proceedings.neurips.cc/paper/2020/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018},
  url={https://arxiv.org/pdf/1804.07461.pdf}
}

@article{liu2019b,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2021-02-05},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archivePrefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf},
  journal = {arXiv:1907.11692 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{wietingkiela2019,
  title={No training required: Exploring random encoders for sentence classification},
  author={Wieting, John and Kiela, Douwe},
  journal={arXiv preprint arXiv:1901.10444},
  year={2019},
  url={https://arxiv.org/pdf/1901.10444.pdf}
}

@inproceedings{ulyanov2018deep,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9446--9454},
  year={2018},
  url={https://arxiv.org/pdf/1711.10925.pdf}
}
@article{frankle2020training,
  title={Training batchnorm and only batchnorm: On the expressive power of random features in cnns},
  author={Frankle, Jonathan and Schwab, David J and Morcos, Ari S},
  journal={arXiv preprint arXiv:2003.00152},
  year={2020},
  url={https://arxiv.org/abs/2003.00152}
}
@inproceedings{xie2019exploring,
  title={Exploring randomly wired neural networks for image recognition},
  author={Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1284--1293},
  year={2019},
  url={https://arxiv.org/pdf/1904.01569.pdf}
}

@article{williams2018latent,
  title={Do latent tree learning models identify meaningful structure in sentences?},
  author={Williams, Adina and Drozdov, Andrew and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={253--267},
  year={2018},
  publisher={MIT Press},
  url={https://www.aclweb.org/anthology/Q18-1019/}
}

@article{scheible2013cutting,
  title={Cutting recursive autoencoder trees},
  author={Scheible, Christian and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1301.2811},
  year={2013},
  url={https://arxiv.org/pdf/1301.2811.pdf}
}

@inproceedings{jain2019attention,
  author={Jain, Sarthak and Wallace, Byron C},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {Attention is not Explanation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {3543--3556},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
}

@article{papadimitriou2020,
  title = {Learning {{Music Helps You Read}}: {{Using Transfer}} to {{Study Linguistic Structure}} in {{Language Models}}},
  shorttitle = {Learning {{Music Helps You Read}}},
  author = {Papadimitriou, Isabel and Jurafsky, Dan},
  year = {2020},
  month = oct,
  url = {http://arxiv.org/abs/2004.14601},
  urldate = {2021-02-12},
  abstract = {We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads to the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.},
  archivePrefix = {arXiv},
  eprint = {2004.14601},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Papadimitriou and Jurafsky - 2020 - Learning Music Helps You Read Using Transfer to S.pdf},
  journal = {arXiv:2004.14601 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{pimentel2020b,
  title = {Pareto {{Probing}}: {{Trading Off Accuracy}} for {{Complexity}}},
  shorttitle = {Pareto {{Probing}}},
  author = {Pimentel, Tiago and Saphra, Naomi and Williams, Adina and Cotterell, Ryan},
  year = {2020},
  month = nov,
  url = {http://arxiv.org/abs/2010.02180},
  urldate = {2021-02-22},
  abstract = {The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the NLP literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations\textemdash e.g., why should the non-contextual fastText representations encode more morpho-syntactic information than the contextual BERT representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations. Our code can be found at https://github. com/rycolab/pareto-probing.},
  archivePrefix = {arXiv},
  eprint = {2010.02180},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Pimentel et al. - 2020 - Pareto Probing Trading Off Accuracy for Complexit.pdf},
  journal = {arXiv:2010.02180 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{sinha2020b,
  title = {{U}nnatural {L}anguage {I}nference},
  author = {Sinha, Koustuv and Parthasarathi, Prasanna and Pineau, Joelle and Williams, Adina},
  year = {2020},
  month = dec,
  url = {http://arxiv.org/abs/2101.00010},
  urldate = {2021-03-27},
  abstract = {Natural Language Understanding has witnessed a watershed moment with the introduction of large pre-trained Transformer networks. These models achieve state-of-the-art on various tasks, notably including Natural Language Inference (NLI). Many studies have shown that the large representation space imbibed by the models encodes some syntactic and semantic information. However, to really ``know syntax'', a model must recognize when its input violates syntactic rules and calculate inferences accordingly. In this work, we find that stateof-the-art NLI models, such as RoBERTa and BART are invariant to, and sometimes even perform better on, examples with randomly reordered words. With iterative search, we are able to construct randomized versions of NLI test sets, which contain permuted hypothesispremise pairs with the same words as the original, yet are classified with perfect accuracy by large pre-trained models, as well as preTransformer state-of-the-art encoders. We find the issue to be language and model invariant, and hence investigate the root cause. To partially alleviate this effect, we propose a simple training methodology. Our findings call into question the idea that our natural language understanding models, and the tasks used for measuring their progress, genuinely require a human-like understanding of syntax.},
  archivePrefix = {arXiv},
  eprint = {2101.00010},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Sinha et al. - 2020 - Unnatural Language Inference.pdf},
  journal = {arXiv:2101.00010 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{wolf2019,
  title = {Some Additional Experiments Extending the Tech Report ''{{Assessing BERT}}'s {{Syntactic Abilities}}'' by {{Yoav Goldberg}}},
  author = {Wolf, Thomas},
  year = {2019},
  pages = {7},
  url={https://huggingface.co/bert-syntax/extending-bert-syntax.pdf},
  language = {en}
}

@article{goldberga,
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  author = {Goldberg, Yoav},
  pages = {4},
  abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ``coloreless green ideas'' subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
  language = {en},
  year={2019},
  journal = {CoRR},
  url={https://arxiv.org/pdf/1901.05287.pdf}
}

@article{gulordava2018,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  month = mar,
  url = {http://arxiv.org/abs/1803.11138},
  urldate = {2021-02-21},
  abstract = {Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (``The colorless green iiddeeaass I ate with the chair sslleeeepp furiously''), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallowpattern extractors, but they also acquire deeper grammatical competence.},
  archivePrefix = {arXiv},
  eprint = {1803.11138},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Gulordava et al. - 2018 - Colorless green recurrent networks dream hierarchi.pdf},
  journal = {arXiv:1803.11138 [cs]},
  language = {en},
  primaryClass = {cs}
}


@article{zhangb,
  title = {{{PAWS}}: {{Paraphrase Adversaries}} from {{Word Scrambling}}},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  pages = {11},
  abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 wellformed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. Stateof-the-art models trained on existing datasets have dismal performance on PAWS ({$<$}40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Zhang et al. - PAWS Paraphrase Adversaries from Word Scrambling.pdf},
  language = {en}
}

@article{pham2020,
  title = {Out of {{Order}}: {{How}} Important Is the Sequential Order of Words in a Sentence in {{Natural Language Understanding}} Tasks?},
  shorttitle = {Out of {{Order}}},
  author = {Pham, Thang M. and Bui, Trung and Mai, Long and Nguyen, Anh},
  year = {2020},
  month = dec,
  abstract = {Do state-of-the-art natural language understanding models care about word order\textemdash one of the most important characteristics of a sequence? Not always! We found 75\% to 90\% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Despite BERT embeddings are famously contextual, the contribution of each individual word to downstream tasks is almost unchanged even after the word's context is shuffled. BERT-based models are able to exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequence-pair inputs in natural language inference) to make correct decisions when tokens are arranged in random orders. Encouraging classifiers to capture word order information improves the performance on most GLUE tasks, SQuAD 2.0 and out-ofsamples. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.},
  archivePrefix = {arXiv},
  eprint = {2012.15180},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Pham et al. - 2020 - Out of Order How important is the sequential orde.pdf},
  journal = {arXiv:2012.15180 [cs]},
  language = {en},
  primaryClass = {cs}
}


@article{maudslay2020a,
  title = {A {{Tale}} of a {{Probe}} and a {{Parser}}},
  author = {Maudslay, Rowan Hall and Valvoda, Josef and Pimentel, Tiago and Williams, Adina and Cotterell, Ryan},
  year = {2020},
  month = may,
  abstract = {Measuring what linguistic information is encoded in neural models of language has become popular in NLP. Researchers approach this enterprise by training "probes" - supervised models designed to extract linguistic structure from another model's output. One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations. The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious. To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend - the structural probe outperforms the parser. This begs the question: which metric should we prefer?},
  archivePrefix = {arXiv},
  eprint = {2005.01641},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Maudslay et al. - 2020 - A Tale of a Probe and a Parser.pdf},
  journal = {arXiv:2005.01641 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{pimentel2020,
  title = {Information-{{Theoretic Probing}} for {{Linguistic Structure}}},
  author = {Pimentel, Tiago and Valvoda, Josef and Maudslay, Rowan Hall and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
  year = {2020},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {4609--4622},
  publisher = {Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/2020.acl-main.420.pdf}
}


@article{perez2021,
  title = {Rissanen {{Data Analysis}}: {{Examining Dataset Characteristics}} via {{Description Length}}},
  shorttitle = {Rissanen {{Data Analysis}}},
  author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  year = {2021},
  month = mar,
  abstract = {We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.},
  archivePrefix = {arXiv},
  eprint = {2103.03872},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Perez et al. - 2021 - Rissanen Data Analysis Examining Dataset Characte.pdf},
  journal = {arXiv:2103.03872 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}


@inproceedings{salazar2020a,
  title = {Masked {{Language Model Scoring}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
  year = {2020},
  pages = {2699--2712},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.240},
  abstract = {Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an endto-end LibriSpeech model's WER by 30\% relative and adds up to +1.7 BLEU on state-of-theart baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https: //github.com/awslabs/mlm-scoring.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Salazar et al. - 2020 - Masked Language Model Scoring2.pdf},
  language = {en}
}

@inproceedings{jawahar2019a,
  title = {What {{Does BERT Learn}} about the {{Structure}} of {{Language}}?},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  year = {2019},
  pages = {3651--3657},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1356},
  abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We first show that BERT's phrasal representation captures phrase-level information in the lower layers. We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua2.pdf},
  language = {en}
}


@article{kataoka2021,
  title = {Pre-Training without {{Natural Images}}},
  author = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
  year = {2021},
  month = jan,
  abstract = {Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning. We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law existing in the background knowledge of the real world. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinite scale dataset of labeled images. Although the models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, does not necessarily outperform models pre-trained with human annotated datasets at all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.},
  archivePrefix = {arXiv},
  eprint = {2101.08515},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Kataoka et al. - 2021 - Pre-training without Natural Images.pdf},
  journal = {arXiv:2101.08515 [cs]},
  language = {en},
  primaryClass = {cs}
}


@article{shen2020a,
  title = {Reservoir {{Transformer}}},
  author = {Shen, Sheng and Baevski, Alexei and Morcos, Ari S. and Keutzer, Kurt and Auli, Michael and Kiela, Douwe},
  year = {2020},
  month = dec,
  abstract = {We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and wellestablished ideas in machine learning, we explore a variety of non-linear ``reservoir'' layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.},
  archivePrefix = {arXiv},
  eprint = {2012.15045},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Shen et al. - 2020 - Reservoir Transformer2.pdf},
  journal = {arXiv:2012.15045 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{mikolov2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  url={https://arxiv.org/pdf/1310.4546.pdf}
}

@misc{clark2019does,
      title={What Does BERT Look At? An Analysis of BERT's Attention},
      author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
      year={2019},
      eprint={1906.04341},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{dozat2016deep,
  title={Deep biaffine attention for neural dependency parsing},
  author={Dozat, Timothy and Manning, Christopher D},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
}

@article{cola_warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  publisher={MIT Press},
  url={https://arxiv.org/pdf/1805.12471.pdf}
}

@inproceedings{sst2_socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013},
  url={https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf}
}

@inproceedings{mrpc_dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005},
  url={https://www.aclweb.org/anthology/I05-5002.pdf}
}

@article{stsb_cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

@article{mnli_williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017},
  url={https://www.aclweb.org/anthology/N18-1101.pdf}
}

@article{qnli_rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016},
  url={https://arxiv.org/pdf/1606.05250.pdf}
}

@article{qnli_2_demszky2018transforming,
  title={Transforming question answering datasets into natural language inference datasets},
  author={Demszky, Dorottya and Guu, Kelvin and Liang, Percy},
  journal={arXiv preprint arXiv:1809.02922},
  year={2018},
  url={https://arxiv.org/pdf/1809.02922.pdf}
}

@inproceedings{rte1_dagan2005pascal,
  title={The {PASCAL} recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  pages={177--190},
  year={2005},
  organization={Springer},
  url={https://link.springer.com/chapter/10.1007/11736790_9}
}

@inproceedings{rte2_haim2006second,
  title={The second {PASCAL} recognising textual entailment challenge},
  author={Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment},
  year={2006},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.8552&rep=rep1&type=pdf}
}

@inproceedings{rte3_giampiccolo2007third,
  title={The third {PASCAL} recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007},
  url={https://www.aclweb.org/anthology/W07-1401.pdf}
}

@inproceedings{rte5_bentivogli2009fifth,
  title={The Fifth {PASCAL} Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle={TAC},
  year={2009},
  url={https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.1231&rep=rep1&type=pdf}
}

@inproceedings{wnli_levesque2012winograd,
  title={The {Winograd} schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012},
  organization={Citeseer}
}

@article{bies2012english,
  title={English web treebank},
  author={Bies, Ann and Mott, Justin and Warner, Colin and Kulick, Seth},
  journal={Linguistic Data Consortium, Philadelphia, PA},
  year={2012},
  url={https://catalog.ldc.upenn.edu/LDC2012T13}
}

@inproceedings{silveira2014gold,
  title={A Gold Standard Dependency Corpus for English.},
  author={Silveira, Natalia and Dozat, Timothy and De Marneffe, Marie-Catherine and Bowman, Samuel R and Connor, Miriam and Bauer, John and Manning, Christopher D},
  booktitle={LREC},
  pages={2897--2904},
  year={2014},
  organization={Citeseer},
  url={http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@misc{wu2016googles,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhang2019paws,
  title={PAWS: Paraphrase adversaries from word scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  journal={arXiv preprint arXiv:1904.01130},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1131.pdf}
}

@inproceedings{alain2016understanding,
  author    = {Guillaume Alain and
               Yoshua Bengio},
  title     = {Understanding intermediate layers using linear classifier probes},
  booktitle = {{ICLR} 2017, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HJ4-rAVtl},
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1419.pdf}
}

@article{ettinger2020,
  title = {What {{BERT Is Not}}: {{Lessons}} from a {{New Suite}} of {{Psycholinguistic Diagnostics}} for {{Language Models}}},
  shorttitle = {What {{BERT Is Not}}},
  author = {Ettinger, Allyson},
  year = {2020},
  month = dec,
  volume = {8},
  pages = {34--48},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00298},
  abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\textemdash and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Ettinger - 2020 - What BERT Is Not Lessons from a New Suite of Psyc.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}

@article{rissanen1984universal,
  title={Universal coding, information, prediction, and estimation},
  author={Rissanen, Jorma},
  journal={IEEE Transactions on Information theory},
  volume={30},
  number={4},
  pages={629--636},
  year={1984},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/1056936}
}

@inproceedings{bansal2019learning,
  title= {Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks},
  author={Bansal, Trapit and Jha, Rishikesh and McCallum, Andrew},
  booktitle = {Proceedings of {COLING} 2020},
  editor    = {Donia Scott and
               N{\'{u}}ria Bel and
               Chengqing Zong},
  pages     = {5108--5123},
  publisher = {International Committee on Computational Linguistics},
  year      = {2020},
}


@article{zhang2021a,
  title = {Revisiting {{Few}}-Sample {{BERT Fine}}-Tuning},
  author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q. and Artzi, Yoav},
  year = {2021},
  month = mar,
  abstract = {This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process.},
  archivePrefix = {arXiv},
  eprint = {2006.05987},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Zhang et al. - 2021 - Revisiting Few-sample BERT Fine-tuning2.pdf},
  journal = {arXiv:2006.05987 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for information science},
  volume={41},
  number={6},
  pages={391--407},
  year={1990},
  publisher={Wiley Online Library},
  url={https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1097-4571(199009)41:6\%3C391::AID-ASI1\%3E3.0.CO;2-9}
}

@article{landauer1997solution,
  title={A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
  author={Landauer, Thomas K and Dumais, Susan T},
  journal={Psychological review},
  volume={104},
  number={2},
  pages={211},
  year={1997},
  publisher={American Psychological Association},
  url={https://psycnet.apa.org/record/1997-03612-001}
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008},
  url={https://dl.acm.org/doi/10.1145/1390156.1390177}
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018},
  url={https://arxiv.org/pdf/1802.05365.pdf}
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis},
  url={https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520}
}

@article{tenney2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019},
  url={https://arxiv.org/pdf/1905.05950.pdf}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7720155/}
}

@article{greenberg1963some,
  title={Some universals of grammar with particular reference to the order of meaningful elements},
  author={Greenberg, Joseph},
  journal={In J. Greenberg, ed., Universals of Language. 73-113. Cambridge, MA.},
  year={1963},
  publisher={mit Press},
  url={https://wals.info/refdb/record/Greenberg-1963}
}

@article{levy2015improving,
  title={Improving distributional similarity with lessons learned from word embeddings},
  author={Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={211--225},
  year={2015},
  publisher={MIT Press},
  url={https://www.aclweb.org/anthology/Q15-1016.pdf}
}


@article{Hahn2347,
  title = {Universals of Word Order Reflect Optimization of Grammars for Efficient Communication},
  author = {Hahn, Michael and Jurafsky, Dan and Futrell, Richard},
  year = {2020},
  volume = {117},
  pages = {2347--2353},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  doi = {10.1073/pnas.1910923117},
  abstract = {Human languages share many grammatical properties. We show that some of these properties can be explained by the need for languages to offer efficient communication between humans given our cognitive constraints. Grammars of languages seem to find a balance between two communicative pressures: to be simple enough to allow the speaker to easily produce sentences, but complex enough to be unambiguous to the hearer, and this balance explains well-known word-order generalizations across our sample of 51 varied languages. Our results offer quantitative and computational evidence that language structure is dynamically shaped by communicative and cognitive pressures.The universal properties of human languages have been the subject of intense study across the language sciences. We report computational and corpus evidence for the hypothesis that a prominent subset of these universal properties\textemdash those related to word order\textemdash result from a process of optimization for efficient communication among humans, trading off the need to reduce complexity with the need to reduce ambiguity. We formalize these two pressures with information-theoretic and neural-network models of complexity and ambiguity and simulate grammars with optimized word-order parameters on large-scale data from 51 languages. Evolution of grammars toward efficiency results in word-order patterns that predict a large subset of the major word-order correlations across languages.},
  eprint = {https://www.pnas.org/content/117/5/2347.full.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  number = {5}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015},
  url={https://arxiv.org/pdf/1506.06724.pdf}
}

@article{dryer1992greenbergian,
  title={The {G}reenbergian word order correlations},
  author={Dryer, Matthew S},
  journal={Language},
  pages={81--138},
  year={1992},
  publisher={JSTOR}
}

@book{cinque1999adverbs,
  title={Adverbs and functional heads: A cross-linguistic perspective},
  author={Cinque, Guglielmo},
  year={1999},
  publisher={Oxford University Press on Demand}
}

@article{belinkov2021probing,
  title={Probing Classifiers: Promises, Shortcomings, and Alternatives},
  author={Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2102.12452},
  year={2021}
}

@article{hupkes2018visualisation,
  title={Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}

@inproceedings{torroba-hennigen-etal-2020-intrinsic,
    title = "Intrinsic Probing through Dimension Selection",
    author = "Torroba Hennigen, Lucas  and
      Williams, Adina  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.15",
    doi = "10.18653/v1/2020.emnlp-main.15",
    pages = "197--216",
    abstract = "Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.",
}

@software{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = {2020},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@book{chomsky1957syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  year={1957},
  publisher={Walter de Gruyter}
}

@article{bowman2021will,
  title={What Will it Take to Fix Benchmarking in Natural Language Understanding?},
  author={Bowman, Samuel R and Dahl, George E},
  journal={arXiv preprint arXiv:2104.02145},
  year={2021}
}

@article{lakretz2021mechanisms,
title = {Mechanisms for handling nested dependencies in neural-network language models and humans},
journal = {Cognition},
pages = {104699},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2021.104699},
url = {https://www.sciencedirect.com/science/article/pii/S0010027721001189},
author = {Yair Lakretz and Dieuwke Hupkes and Alessandra Vergallito and Marco Marelli and Marco Baroni and Stanislas Dehaene},
}

@inproceedings{papadimitriou-etal-2021-deep,
    title = "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual {BERT}",
    author = "Papadimitriou, Isabel  and
      Chi, Ethan A.  and
      Futrell, Richard  and
      Mahowald, Kyle",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.215",
    pages = "2522--2532",
    abstract = "We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a {``}subject{''}) is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.",
}

@inproceedings{conneau-kiela-2018-senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1269",
}

@inproceedings{conneau-etal-2018-cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
}
@article{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}
@article{jia2016,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  author = {Jia, Robin and Liang, Percy},
  year = {2016}
}
@article{qiu2020,
  title = {Pre-Trained {{Models}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Pre-Trained {{Models}} for {{Natural Language Processing}}},
  author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  year = {2020},
  month = oct,
  journal = {Science China Technological Sciences},
  volume = {63},
  number = {10},
  eprint = {2003.08271},
  eprinttype = {arxiv},
  pages = {1872--1897},
  issn = {1674-7321, 1869-1900},
  doi = {10.1007/s11431-020-1647-3},
  abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
  archiveprefix = {arXiv},
  language = {en},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Qiu et al. - 2020 - Pre-trained Models for Natural Language Processing.pdf}
}

@inproceedings{jin2020bert,
  title={Is bert really robust? a strong baseline for natural language attack on text classification and entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8018--8025},
  year={2020}
}

@article{kaushik2018much,
  title={How much reading does reading comprehension require? a critical investigation of popular benchmarks},
  author={Kaushik, Divyansh and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1808.04926},
  year={2018}
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@article{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{kim2014convolutional,
      title={Convolutional Neural Networks for Sentence Classification},
      author={Yoon Kim},
      year={2014},
      eprint={1408.5882},
      journal={EMNLP},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}
