%\nonstopmode
\documentclass[12pt]{article}
%\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=0.78in]{geometry}
\newcommand{\argmax}{\operatornamewithlimits{\mathbb{argmax}}}
\usepackage{amssymb}
\usepackage{breqn}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{amsmath,eqnarray}
\usepackage[svgnames]{xcolor}
\definecolor{citecol}{RGB}{0,12,128}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[subrefformat=parens,labelformat=parens]{subfig}
\usepackage{float}
\usepackage{mathptmx}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
%\usepackage[backend=bibtex,style=alphabetic,citestyle=authoryear]{biblatex}
\usepackage[numbers]{natbib}
\bibliographystyle{abbrv}
% \usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{hyperref}


\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}

\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\vd}[0]{\vect{d}}
\newcommand{\ve}[0]{\vect{e}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vp}[0]{\vect{p}}
\newcommand{\vq}[0]{\vect{q}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vz}[0]{\vect{z}}


\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mH}[0]{\matr{H}}
\newcommand{\mM}[0]{\matr{M}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mR}[0]{\matr{R}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mW}[0]{\matr{W}}

\def\Snospace~{\S{}} % If i don't add this overleaf complains
\renewcommand{\sectionautorefname}{\Snospace}
\renewcommand{\subsectionautorefname}{\Snospace}

%\hypersetup{colorlinks=true,citecolor=citecol}
%\DeclareMathOperator*{\argmax}{arg\,max}

%\addbibresource{syllabus.bib}

\title{Systematic language understanding: a study on the capabilities and limits of language understanding by modern neural networks}
\author{Koustuv Sinha \\ Ph.D. Proposal Document}
\date{September 2021}

\begin{document}

\maketitle

\section{Introduction}

Language allows us to express and comprehend a vast variety of novel thoughts and ideas. Through language, humans exhibit higher-order reasoning and comprehension. Thus, to develop models which mimic human-like reasoning, a principled focus in computer science research is to develop models which understand and reason on natural language. To foster research in developing such state-of-the-art natural language understanding (NLU) models, several datasets and tasks on reading comprehension have been proposed in recent literature. These include tasks such as question answering (QA), natural language inference (NLI), commonsense reasoning to name a few. Over the last decade, several advancements have been made to develop such models, the most successful ones till date involve deep neural models, especially Transformers \CITE, a class of multi-head self-attention models. Since its introduction in 2017, Transformer-based models have achieved impressive results on numerous benchmarks and datasets, with BERT \CITE being one of the most popular instantiation of the same. Using a technique known as ``pre-training'', Transformer-based models are first trained to replicate massive corpus of text. Through this kind of unsupervised training, the models learn and tune their millions and billions of parameters, and using which they solve NLU datasets with surprising, near-human efficiency \CITE.

While Transformer-based models excel in these datasets, it is less clear why do they work so well. Due to the sheer amount of overparameterization, direct inspection of the inner workings of these models are limited. Thus, various research have been conducted by using auxilliary tasks and probing functions to understand the reasoning processes employed by these models \cite{rogers2020}. It has been claimed in the literature that BERT embeddings contain syntactic information about a given sentence, to the extent that the model may internally perform several natural language processing pipeline steps, involving parts-of-speech tagging, entity recognition etc \CITE. BERT has also been credited to acquire some level of semantic understanding \CITE, and contains relevant information about relations and world knowledge \CITE. All of these results indicate to the fact that purely pre-training with massive overparameterized models and large corpora might just be the perfect roadmap to achieve ``human-like'' reasoning capabilities.

On the other hand, there have been growing concerns regarding the ability of these NLU models to understand language in a ``systematic'' and robust way. The phenomenon of \textit{systematicity}, widely studied in the cognitive sciences, refers to the fact that lexical units such as words make consistent contributions to the meaning of the sentences in which they appear \CITE[Fodor]. As an illustration, they provide an example that all English speakers who understand the sentence ``John loves the girl'' should also understand the phrase ``the girl loves John''. In case of NLU tasks, this accounts to model being consistent in understanding novel compositions of existing, learned words or phrases. However, there is growing evidence in literature which highlight the brittleness of NLU systems to such adversarial examples \CITE. More so, there is strong evidence that state-of-the-art NLU models tend to exploit statistical artifacts in datasets, rather than exhibiting true reasoning and generalization capabilities \CITE.

In view of the positive and negative evidences towards Transformers acquiring ``human-like'' natural language understanding capacity, it is very important that we take a step back and carefully examine the reasoning processes of the NLU models in the view of systematicity and robustness. Since these NLU models are now being deployed in production and decision making systems, it is even more prudent to test the models towards systematic understanding in order to avoid catastrophic scenarios. In this proposal, I thus discuss my work till now in my doctoral studies to understand the limits of systematic and robust natural language understanding of NLU models. Concretely, first I discuss our proposed systematicity tests on artificial and natural languages by using first-order logic (FOL), and what we learned from the model using such tests. This involves the following two papers:

\begin{itemize}
  \item \textit{CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text}, published at EMNLP 2019 (Oral presentation)
  \item \textit{Probing Linguistic Systematicity} \footnote{Work done as second author.}, published at ACL 2020
\end{itemize}

Secondly, I discuss our work on understanding the limits of systematicity of NLU models by subjecting these models to scrambled word order sentences, involving the following two papers:

\begin{itemize}
  \item \textit{UnNatural Language Inference}, published at ACL 2021 (Oral presentation, Outstanding paper award) \cite{sinha2021}
  \item \textit{Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little}, submitted to EMNLP 2021 \cite{sinha2021a}
\end{itemize}

This document does not discuss concurrent works on understanding systematic reasoning for proof generation \cite{gontier2020} (published at NeurIPS 2020), proposed dataset on systematic reasoning on graph neural networks \cite{sinha2020c}, or an unreferenced automatic dialog evaluation framework \cite{sinha2020d} (published at ACL 2020) conducted during my doctoral studies.

% TODO mostly copy/rephrase stuff from my previous papers. Start with a general introduction

% TODO define systematicity

% TODO state why we need to investigate systematicity of NLU models - cite the problem papers

% Related work

% Connect systematicity to generalization

\section{Background}

% Language Models

\subsection{Language Models}

%Transformers
% LSTM-based LM

% The rise of pre-training

\subsection{The rise of pre-training}

\section{Contribution 1: Investigating systematicity of NLU models using first order logic}
\label{sec:cont1}

% CLUTRR

In this section, we first talk about our paper, CLUTRR \cite{sinha2019a}. Then, we talk about Probing Linguistic Systematicity \cite{goodwin2020} paper.

% Probing Linguistic Systematicity

\section{Contribution 2: Probing systematicity of pre-trained models using word order}
\label{sec:cont2}

% UNLI
% Unnatural Pre-training

Here, we talk about two related contributions, UnNatural Language Inference \cite{sinha2021} and pretraining with unnatural languages \cite{sinha2021a}.



\section{Future Work \& Timeline}

Until now, most of the work in my doctoral studies have been focused on developing methods to detect the issue of systematicity plaguing neural NLU models. To complete my thesis, I thereby plan to work towards a couple of methods to improve robustness and systematicity of NLU models.

\subsection{Unsupervised syntax learning by mutually exclusive training using word order}

In our prior work on word order (Section \ref{sec:cont2}), we observed that NLU models are largely distributional - they understand the collection of words in a sentence but have limited understanding on the order of words. This poses a problem - representations of random permutations of the sentence having no grounded meaning will still be identified by the NLU models to contain syntactic and semantic information. Due to this distributional effect, we posit that syntax understanding of NLU models are still primitive, mostly restricted to higher order information. Thus, it is imperative to develop mechanisms to imbibe the required syntactical information within the sentence representation, such that it is systematic. One can use syntactic features such as dependency parses to imbibe information about syntax in the sentence representation using auxilliary supervison loss. In literature, such syntactical information has shown to be effective in downstream tasks, such as Relation Extraction (RE) \cite{fundel2007relex}, named entity recognition (NER) \cite{jie2019dependency} and semantic role labeling (SRL) \cite{strubell2018}. More recently, syntax trees are used during pre-training of Transformer based models to imbibe better syntactical information by early and late fusion \cite{sachan2021}. However, such direct supervision models to imbibe syntactical information is difficult as it requires access to preferably human-annotated syntax parses of sentences, which raises questions on the viability of such approaches for real world applications. Even so, limited studies have been performed to investigate systematicity issues of those models trained with supplementary syntactical signal.

Therefore, we propose an alternate, unsupervised mechanism to imbibe syntax information within the sentence representations by leveraging word order. Concretely, we use an auxilliary objective to the model to recognize correct and incorrect permutations of a given sentence alongside the task objective. Now, in the strictest sense there is only one correct ordering of a sentence which conveys the intended meaning. However, natural language (English) allows for a degree of flexibility in word order. Thus, we plan to leverage the idea of \textit{separable permutations} \cite{stanojevic2021}, where a subset of permutations can be treated as positive signal which can be reconstructed from the CCG parse of the given sentence. This auxilliary training loss could potentially inform the model to be systematic in understanding syntax, and thereby reduce the distributional, bag-of-words behavior of the encoder representations.

% Note: potential question from comittee: While this approach still requires a CCG parser, due to the nature of separable permutations this parser need not be perfect.

\subsection{Entity agnostic training for better systematic generalization}

\subsection{Timeline}

\noindent \textbf{Thesis preparation and submission} Expected defense date Fall 2022


\bibliography{main}

\end{document}
