\relax 
\bibstyle{abbrv}
\citation{rogers2020}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{sinha2019a}
\citation{goodwin2020}
\citation{gontier2020}
\citation{sinha2021}
\citation{sinha2021a}
\citation{parthasarathi2021a}
\citation{sinha2020c}
\citation{sinha2020d}
\citation{bordes2013translating}
\citation{bengio2013representation}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background: Natural language understanding through Neural approaches}{2}\protected@file@percent }
\citation{vaswani-etal-2017-attention}
\citation{radford2018improving}
\citation{collobert}
\citation{mikolov}
\citation{glove}
\citation{peters}
\citation{ULMfit}
\citation{devlin}
\citation{qiu2020}
\citation{rogers2020}
\citation{hupkes2018visualisation}
\citation{rogers2020}
\citation{jia2016}
\citation{gururangan2018a}
\citation{mccoy2019}
\citation{hinton1986learning,muggleton1991inductive}
\@writefile{toc}{\contentsline {section}{\numberline {3}Contribution 1: Investigating systematicity of NLU models using first order logic}{4}\protected@file@percent }
\newlabel{sec:cont1}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces CLUTRR inductive reasoning task\relax }}{4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:clutrr_data}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dataset}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CLUTRR dataset design steps\relax }}{4}\protected@file@percent }
\newlabel{fig:clutrr_data_design}{{2}{4}}
\citation{hochreiter1997long,cho2014learning}
\citation{santoro2017simple}
\citation{hudson2018compositional}
\citation{devlin2018bert}
\citation{Velickovic2017-mh}
\citation{lake2017generalization,2018arXiv181107017S}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Systematic generalization results on CLUTRR, when trained on stories of length $k=2,3,4$\relax }}{5}\protected@file@percent }
\newlabel{fig:clutrr_sys_gen_234}{{3}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Testing the robustness of the various models when training and testing on stories containing various types of noise facts.\relax }}{6}\protected@file@percent }
\newlabel{tab:robust}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Noise generation methods in CLUTRR\relax }}{6}\protected@file@percent }
\newlabel{fig:clutrr_data_noise}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{6}\protected@file@percent }
\citation{goodwin2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Related Works}{7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces A template for sentences in the artificial language. Each sentence fills the obligatory positions 1, 3, and 6 with a word: a quantifier, noun, and verb. Optional positions (2, 4 and 5) are filled by either a word (adjective, postmodifier or negation) or by the empty string. Closed-class categories (Quantifiers, adjectives, post modifiers and negation) do not include novel words, while open-class categories (nouns and verbs) includes novel words that are only exposed in the test set. \relax }}{7}\protected@file@percent }
\newlabel{tbl:artlang}{{2}{7}}
\citation{gontier2020}
\citation{Evans2017-pu}
\citation{vaswani2017attention}
\citation{vaswani-etal-2017-attention}
\citation{liu-et-al-2019-roberta}
\citation{lewis-etal-2020-bart}
\citation{radford-etal-2019-language,brown-etal-2020-gpt3}
\citation{wang-etal-2018-glue,wang-etal-2019-superglue}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert,warstadt-bowman-2020-can,wu-etal-2020-perturbed}
\citation{rogers2020}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Systematic generalization issues in proof generation\relax }}{8}\protected@file@percent }
\newlabel{fig:proof_sys_gen}{{5}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Contribution 2: Probing systematicity of pre-trained models using word order}{8}\protected@file@percent }
\newlabel{sec:cont2}{{4}{8}}
\citation{sinha2021,sinha2021a}
\citation{sinha2021}
\citation{chomsky-1995-minimalist}
\citation{harris-1954-distributional}
\citation{cattell-1886-time,scheerer1981early,toyota-2001-changes,baddeley-etal-2009-working,snell-grainger-2017-sentence,snell2019word,wen-etal-2019-parallel}
\citation{heim-kratzer-1998-semantics}
\citation{bender-koller-2020-climbing}
\citation{condoravdi-etal-2003-entailment,dagan-etal-2005-pascal}
\citation{bowman-etal-2015-large}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{nie-etal-2020-adversarial}
\citation{hu-etal-2020-ocnli}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}UnNatural Language Inference}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Motivation}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Experiments and Results}{9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Examples from the MNLI Matched development set. Both the original example and the permuted one elicit the same classification label (entailment and contradiction respectively) from RoBERTa (large).\relax }}{9}\protected@file@percent }
\newlabel{tab:example}{{3}{9}}
\citation{sinha2021a}
\citation{hu-etal-2020-ocnli}
\citation{mollica-2020-composition}
\citation{gupta-etal-2021-bert}
\citation{dasgupta-etal-2018-evaluating,poliak-etal-2018-hypothesis,gururangan-etal-2018-annotation,naik-etal-2019-exploring}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model confidences on unnatural input\relax }}{10}\protected@file@percent }
\newlabel{fig:all_entropy}{{6}{10}}
\citation{sinha2021a}
\citation{liu2019b}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Discussion}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Masked Language Modeling and the Distributional Hypothesis: \textit  {Order word matters pre-training for little}}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Experiments \& Results}{11}\protected@file@percent }
\citation{zhu2015aligning}
\citation{liu2019b}
\citation{sinha2021a}
\citation{wang2018glue}
\citation{cola_warstadt2019neural}
\citation{sst2_socher2013recursive}
\citation{mrpc_dolan2005automatically}
\citation{williams-etal-2018-broad}
\citation{qnli_rajpurkar2016squad,qnli_2_demszky2018transforming}
\citation{rte1_dagan2005pascal,rte2_haim2006second,rte3_giampiccolo2007third,rte5_bentivogli2009fifth}
\citation{zhang2019paws}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Fine-tuning results on various word shuffled pre-trained models.\relax }}{12}\protected@file@percent }
\newlabel{fig:masked_results}{{7}{12}}
\citation{pham2020}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{conneau-etal-2018-cram}
\citation{conneau-kiela-2018-senteval}
\citation{jawahar2019a}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted,gulordava2018,goldberga,wolf2019}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted}
\citation{chomsky1957syntactic}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Unlabeled Attachment Score (UAS) (mean and std) on the dependency parsing task (DEP) on two datasets, UD EWT and PTB, using the Pareto Probing framework \cite  {pimentel2020b}.\relax }}{13}\protected@file@percent }
\newlabel{tab:pareto_dependency}{{4}{13}}
\citation{fundel2007relex}
\citation{jie2019dependency}
\citation{strubell2018}
\citation{sachan2021}
\citation{stanojevic2021}
\citation{nie2020}
\citation{gururangan2018a}
\citation{maudslay2020b}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discussion \& Conclusion}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Future Work \& Timeline}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Unsupervised syntax learning by mutually exclusive training using word order}{14}\protected@file@percent }
\citation{mccoy2019}
\bibdata{main}
\bibcite{baddeley-etal-2009-working}{{1}{}{{}}{{}}}
\bibcite{bender-koller-2020-climbing}{{2}{}{{}}{{}}}
\bibcite{bengio2013representation}{{3}{}{{}}{{}}}
\bibcite{rte5_bentivogli2009fifth}{{4}{}{{}}{{}}}
\bibcite{bordes2013translating}{{5}{}{{}}{{}}}
\bibcite{bowman-etal-2015-large}{{6}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Nonsensical data augmentation for better systematic generalization}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Timeline}{15}\protected@file@percent }
\bibcite{brown-etal-2020-gpt3}{{7}{}{{}}{{}}}
\bibcite{cattell-1886-time}{{8}{}{{}}{{}}}
\bibcite{cho2014learning}{{9}{}{{}}{{}}}
\bibcite{chomsky1957syntactic}{{10}{}{{}}{{}}}
\bibcite{chomsky-1995-minimalist}{{11}{}{{}}{{}}}
\bibcite{condoravdi-etal-2003-entailment}{{12}{}{{}}{{}}}
\bibcite{conneau-kiela-2018-senteval}{{13}{}{{}}{{}}}
\bibcite{conneau-etal-2018-cram}{{14}{}{{}}{{}}}
\bibcite{dagan-etal-2005-pascal}{{15}{}{{}}{{}}}
\bibcite{rte1_dagan2005pascal}{{16}{}{{}}{{}}}
\bibcite{dasgupta-etal-2018-evaluating}{{17}{}{{}}{{}}}
\bibcite{qnli_2_demszky2018transforming}{{18}{}{{}}{{}}}
\bibcite{devlin2018bert}{{19}{}{{}}{{}}}
\bibcite{mrpc_dolan2005automatically}{{20}{}{{}}{{}}}
\bibcite{Evans2017-pu}{{21}{}{{}}{{}}}
\bibcite{fundel2007relex}{{22}{}{{}}{{}}}
\bibcite{rte3_giampiccolo2007third}{{23}{}{{}}{{}}}
\bibcite{goldberga}{{24}{}{{}}{{}}}
\bibcite{gontier2020}{{25}{}{{}}{{}}}
\bibcite{goodwin2020}{{26}{}{{}}{{}}}
\bibcite{gulordava2018}{{27}{}{{}}{{}}}
\bibcite{gupta-etal-2021-bert}{{28}{}{{}}{{}}}
\bibcite{gururangan-etal-2018-annotation}{{29}{}{{}}{{}}}
\bibcite{gururangan2018a}{{30}{}{{}}{{}}}
\bibcite{rte2_haim2006second}{{31}{}{{}}{{}}}
\bibcite{harris-1954-distributional}{{32}{}{{}}{{}}}
\bibcite{heim-kratzer-1998-semantics}{{33}{}{{}}{{}}}
\bibcite{hewitt-manning-2019-structural}{{34}{}{{}}{{}}}
\bibcite{hinton1986learning}{{35}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{36}{}{{}}{{}}}
\bibcite{hu-etal-2020-ocnli}{{37}{}{{}}{{}}}
\bibcite{hudson2018compositional}{{38}{}{{}}{{}}}
\bibcite{jawahar-etal-2019-bert}{{39}{}{{}}{{}}}
\bibcite{jawahar2019a}{{40}{}{{}}{{}}}
\bibcite{jie2019dependency}{{41}{}{{}}{{}}}
\bibcite{lake2017generalization}{{42}{}{{}}{{}}}
\bibcite{lewis-etal-2020-bart}{{43}{}{{}}{{}}}
\bibcite{linzen-etal-2016-assessing}{{44}{}{{}}{{}}}
\bibcite{liu-et-al-2019-roberta}{{45}{}{{}}{{}}}
\bibcite{liu2019b}{{46}{}{{}}{{}}}
\bibcite{marvin-linzen-2018-targeted}{{47}{}{{}}{{}}}
\bibcite{maudslay2020b}{{48}{}{{}}{{}}}
\bibcite{mccoy2019}{{49}{}{{}}{{}}}
\bibcite{mollica-2020-composition}{{50}{}{{}}{{}}}
\bibcite{muggleton1991inductive}{{51}{}{{}}{{}}}
\bibcite{naik-etal-2019-exploring}{{52}{}{{}}{{}}}
\bibcite{nie-etal-2020-adversarial}{{53}{}{{}}{{}}}
\bibcite{nie2020}{{54}{}{{}}{{}}}
\bibcite{parthasarathi2021a}{{55}{}{{}}{{}}}
\bibcite{pham2020}{{56}{}{{}}{{}}}
\bibcite{pimentel2020b}{{57}{}{{}}{{}}}
\bibcite{poliak-etal-2018-hypothesis}{{58}{}{{}}{{}}}
\bibcite{radford2018improving}{{59}{}{{}}{{}}}
\bibcite{radford-etal-2019-language}{{60}{}{{}}{{}}}
\bibcite{qnli_rajpurkar2016squad}{{61}{}{{}}{{}}}
\bibcite{rogers2020}{{62}{}{{}}{{}}}
\bibcite{sachan2021}{{63}{}{{}}{{}}}
\bibcite{santoro2017simple}{{64}{}{{}}{{}}}
\bibcite{scheerer1981early}{{65}{}{{}}{{}}}
\bibcite{sinha2021a}{{66}{}{{}}{{}}}
\bibcite{sinha2021}{{67}{}{{}}{{}}}
\bibcite{sinha2020d}{{68}{}{{}}{{}}}
\bibcite{sinha2019a}{{69}{}{{}}{{}}}
\bibcite{sinha2020c}{{70}{}{{}}{{}}}
\bibcite{snell-grainger-2017-sentence}{{71}{}{{}}{{}}}
\bibcite{snell2019word}{{72}{}{{}}{{}}}
\bibcite{sst2_socher2013recursive}{{73}{}{{}}{{}}}
\bibcite{2018arXiv181107017S}{{74}{}{{}}{{}}}
\bibcite{stanojevic2021}{{75}{}{{}}{{}}}
\bibcite{strubell2018}{{76}{}{{}}{{}}}
\bibcite{toyota-2001-changes}{{77}{}{{}}{{}}}
\bibcite{vaswani2017attention}{{78}{}{{}}{{}}}
\bibcite{vaswani-etal-2017-attention}{{79}{}{{}}{{}}}
\bibcite{Velickovic2017-mh}{{80}{}{{}}{{}}}
\bibcite{wang-etal-2019-superglue}{{81}{}{{}}{{}}}
\bibcite{wang-etal-2018-glue}{{82}{}{{}}{{}}}
\bibcite{wang2018glue}{{83}{}{{}}{{}}}
\bibcite{warstadt-bowman-2020-can}{{84}{}{{}}{{}}}
\bibcite{cola_warstadt2019neural}{{85}{}{{}}{{}}}
\bibcite{wen-etal-2019-parallel}{{86}{}{{}}{{}}}
\bibcite{williams-etal-2018-broad}{{87}{}{{}}{{}}}
\bibcite{wolf2019}{{88}{}{{}}{{}}}
\bibcite{wu-etal-2020-perturbed}{{89}{}{{}}{{}}}
\bibcite{zhang2019paws}{{90}{}{{}}{{}}}
\bibcite{zhu2015aligning}{{91}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
