\relax 
\bibstyle{abbrv}
\citation{rogers2020}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{sinha2019a}
\citation{goodwin2020}
\citation{gontier2020}
\citation{sinha2021}
\citation{sinha2021a}
\citation{parthasarathi2021a}
\citation{sinha2020c}
\citation{sinha2020d}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Language Models}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The rise of pre-training}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Contribution 1: Investigating systematicity of NLU models using first order logic}{2}\protected@file@percent }
\newlabel{sec:cont1}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation}{2}\protected@file@percent }
\citation{hinton1986learning,muggleton1991inductive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces CLUTRR inductive reasoning task\relax }}{3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:clutrr_data}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dataset}{3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CLUTRR dataset design steps\relax }}{3}\protected@file@percent }
\newlabel{fig:clutrr_data_design}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments}{3}\protected@file@percent }
\citation{hochreiter1997long,cho2014learning}
\citation{santoro2017simple}
\citation{hudson2018compositional}
\citation{devlin2018bert}
\citation{Velickovic2017-mh}
\citation{lake2017generalization,2018arXiv181107017S}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Systematic generalization results on CLUTRR, when trained on stories of length $k=2,3,4$\relax }}{4}\protected@file@percent }
\newlabel{fig:clutrr_sys_gen_234}{{3}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Testing the robustness of the various models when training and testing on stories containing various types of noise facts.\relax }}{5}\protected@file@percent }
\newlabel{tab:robust}{{1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Noise generation methods in CLUTRR\relax }}{5}\protected@file@percent }
\newlabel{fig:clutrr_data_noise}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{5}\protected@file@percent }
\citation{goodwin2020}
\citation{gontier2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Related Works}{6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces A template for sentences in the artificial language. Each sentence fills the obligatory positions 1, 3, and 6 with a word: a quantifier, noun, and verb. Optional positions (2, 4 and 5) are filled by either a word (adjective, postmodifier or negation) or by the empty string. Closed-class categories (Quantifiers, adjectives, post modifiers and negation) do not include novel words, while open-class categories (nouns and verbs) includes novel words that are only exposed in the test set. \relax }}{6}\protected@file@percent }
\newlabel{tbl:artlang}{{2}{6}}
\citation{Evans2017-pu}
\citation{vaswani2017attention}
\citation{vaswani-etal-2017-attention}
\citation{liu-et-al-2019-roberta}
\citation{lewis-etal-2020-bart}
\citation{radford-etal-2019-language,brown-etal-2020-gpt3}
\citation{wang-etal-2018-glue,wang-etal-2019-superglue}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert,warstadt-bowman-2020-can,wu-etal-2020-perturbed}
\citation{rogers2020}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Systematic generalization issues in proof generation\relax }}{7}\protected@file@percent }
\newlabel{fig:proof_sys_gen}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Contribution 2: Probing systematicity of pre-trained models using word order}{7}\protected@file@percent }
\newlabel{sec:cont2}{{4}{7}}
\citation{sinha2021,sinha2021a}
\citation{sinha2021}
\citation{chomsky-1995-minimalist}
\citation{harris-1954-distributional}
\citation{cattell-1886-time,scheerer1981early,toyota-2001-changes,baddeley-etal-2009-working,snell-grainger-2017-sentence,snell2019word,wen-etal-2019-parallel}
\citation{heim-kratzer-1998-semantics}
\citation{bender-koller-2020-climbing}
\citation{condoravdi-etal-2003-entailment,dagan-etal-2005-pascal}
\citation{bowman-etal-2015-large}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{nie-etal-2020-adversarial}
\citation{hu-etal-2020-ocnli}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}UnNatural Language Inference}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Motivation}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Experiments and Results}{8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Examples from the MNLI Matched development set. Both the original example and the permuted one elicit the same classification label (entailment and contradiction respectively) from RoBERTa (large).\relax }}{8}\protected@file@percent }
\newlabel{tab:example}{{3}{8}}
\citation{sinha2021a}
\citation{hu-etal-2020-ocnli}
\citation{mollica-2020-composition}
\citation{gupta-etal-2021-bert}
\citation{dasgupta-etal-2018-evaluating,poliak-etal-2018-hypothesis,gururangan-etal-2018-annotation,naik-etal-2019-exploring}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model confidences on unnatural input\relax }}{9}\protected@file@percent }
\newlabel{fig:all_entropy}{{6}{9}}
\citation{sinha2021a}
\citation{liu2019b}
\citation{zhu2015aligning}
\citation{liu2019b}
\citation{sinha2021a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Discussion}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Masked Language Modeling and the Distributional Hypothesis: \textit  {Order word matters pre-training for little}}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Experiments \& Results}{10}\protected@file@percent }
\citation{wang2018glue}
\citation{cola_warstadt2019neural}
\citation{sst2_socher2013recursive}
\citation{mrpc_dolan2005automatically}
\citation{williams-etal-2018-broad}
\citation{qnli_rajpurkar2016squad,qnli_2_demszky2018transforming}
\citation{rte1_dagan2005pascal,rte2_haim2006second,rte3_giampiccolo2007third,rte5_bentivogli2009fifth}
\citation{zhang2019paws}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Fine-tuning results on various word shuffled pre-trained models.\relax }}{11}\protected@file@percent }
\newlabel{fig:masked_results}{{7}{11}}
\citation{pham2020}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{conneau-etal-2018-cram}
\citation{conneau-kiela-2018-senteval}
\citation{jawahar2019a}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted,gulordava2018,goldberga,wolf2019}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted}
\citation{chomsky1957syntactic}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Unlabeled Attachment Score (UAS) (mean and std) on the dependency parsing task (DEP) on two datasets, UD EWT and PTB, using the Pareto Probing framework \cite  {pimentel2020b}.\relax }}{12}\protected@file@percent }
\newlabel{tab:pareto_dependency}{{4}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discussion \& Conclusion}{12}\protected@file@percent }
\citation{fundel2007relex}
\citation{jie2019dependency}
\citation{strubell2018}
\citation{sachan2021}
\citation{stanojevic2021}
\citation{nie2020}
\citation{gururangan2018a}
\citation{maudslay2020b}
\@writefile{toc}{\contentsline {section}{\numberline {5}Future Work \& Timeline}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Unsupervised syntax learning by mutually exclusive training using word order}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Nonsensical data augmentation for better systematic generalization}{13}\protected@file@percent }
\citation{mccoy2019}
\bibdata{main}
\bibcite{baddeley-etal-2009-working}{{1}{}{{}}{{}}}
\bibcite{bender-koller-2020-climbing}{{2}{}{{}}{{}}}
\bibcite{rte5_bentivogli2009fifth}{{3}{}{{}}{{}}}
\bibcite{bowman-etal-2015-large}{{4}{}{{}}{{}}}
\bibcite{brown-etal-2020-gpt3}{{5}{}{{}}{{}}}
\bibcite{cattell-1886-time}{{6}{}{{}}{{}}}
\bibcite{cho2014learning}{{7}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Timeline}{14}\protected@file@percent }
\bibcite{chomsky1957syntactic}{{8}{}{{}}{{}}}
\bibcite{chomsky-1995-minimalist}{{9}{}{{}}{{}}}
\bibcite{condoravdi-etal-2003-entailment}{{10}{}{{}}{{}}}
\bibcite{conneau-kiela-2018-senteval}{{11}{}{{}}{{}}}
\bibcite{conneau-etal-2018-cram}{{12}{}{{}}{{}}}
\bibcite{dagan-etal-2005-pascal}{{13}{}{{}}{{}}}
\bibcite{rte1_dagan2005pascal}{{14}{}{{}}{{}}}
\bibcite{dasgupta-etal-2018-evaluating}{{15}{}{{}}{{}}}
\bibcite{qnli_2_demszky2018transforming}{{16}{}{{}}{{}}}
\bibcite{devlin2018bert}{{17}{}{{}}{{}}}
\bibcite{mrpc_dolan2005automatically}{{18}{}{{}}{{}}}
\bibcite{Evans2017-pu}{{19}{}{{}}{{}}}
\bibcite{fundel2007relex}{{20}{}{{}}{{}}}
\bibcite{rte3_giampiccolo2007third}{{21}{}{{}}{{}}}
\bibcite{goldberga}{{22}{}{{}}{{}}}
\bibcite{gontier2020}{{23}{}{{}}{{}}}
\bibcite{goodwin2020}{{24}{}{{}}{{}}}
\bibcite{gulordava2018}{{25}{}{{}}{{}}}
\bibcite{gupta-etal-2021-bert}{{26}{}{{}}{{}}}
\bibcite{gururangan-etal-2018-annotation}{{27}{}{{}}{{}}}
\bibcite{gururangan2018a}{{28}{}{{}}{{}}}
\bibcite{rte2_haim2006second}{{29}{}{{}}{{}}}
\bibcite{harris-1954-distributional}{{30}{}{{}}{{}}}
\bibcite{heim-kratzer-1998-semantics}{{31}{}{{}}{{}}}
\bibcite{hewitt-manning-2019-structural}{{32}{}{{}}{{}}}
\bibcite{hinton1986learning}{{33}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{34}{}{{}}{{}}}
\bibcite{hu-etal-2020-ocnli}{{35}{}{{}}{{}}}
\bibcite{hudson2018compositional}{{36}{}{{}}{{}}}
\bibcite{jawahar-etal-2019-bert}{{37}{}{{}}{{}}}
\bibcite{jawahar2019a}{{38}{}{{}}{{}}}
\bibcite{jie2019dependency}{{39}{}{{}}{{}}}
\bibcite{lake2017generalization}{{40}{}{{}}{{}}}
\bibcite{lewis-etal-2020-bart}{{41}{}{{}}{{}}}
\bibcite{linzen-etal-2016-assessing}{{42}{}{{}}{{}}}
\bibcite{liu-et-al-2019-roberta}{{43}{}{{}}{{}}}
\bibcite{liu2019b}{{44}{}{{}}{{}}}
\bibcite{marvin-linzen-2018-targeted}{{45}{}{{}}{{}}}
\bibcite{maudslay2020b}{{46}{}{{}}{{}}}
\bibcite{mccoy2019}{{47}{}{{}}{{}}}
\bibcite{mollica-2020-composition}{{48}{}{{}}{{}}}
\bibcite{muggleton1991inductive}{{49}{}{{}}{{}}}
\bibcite{naik-etal-2019-exploring}{{50}{}{{}}{{}}}
\bibcite{nie-etal-2020-adversarial}{{51}{}{{}}{{}}}
\bibcite{nie2020}{{52}{}{{}}{{}}}
\bibcite{parthasarathi2021a}{{53}{}{{}}{{}}}
\bibcite{pham2020}{{54}{}{{}}{{}}}
\bibcite{pimentel2020b}{{55}{}{{}}{{}}}
\bibcite{poliak-etal-2018-hypothesis}{{56}{}{{}}{{}}}
\bibcite{radford-etal-2019-language}{{57}{}{{}}{{}}}
\bibcite{qnli_rajpurkar2016squad}{{58}{}{{}}{{}}}
\bibcite{rogers2020}{{59}{}{{}}{{}}}
\bibcite{sachan2021}{{60}{}{{}}{{}}}
\bibcite{santoro2017simple}{{61}{}{{}}{{}}}
\bibcite{scheerer1981early}{{62}{}{{}}{{}}}
\bibcite{sinha2021a}{{63}{}{{}}{{}}}
\bibcite{sinha2021}{{64}{}{{}}{{}}}
\bibcite{sinha2020d}{{65}{}{{}}{{}}}
\bibcite{sinha2019a}{{66}{}{{}}{{}}}
\bibcite{sinha2020c}{{67}{}{{}}{{}}}
\bibcite{snell-grainger-2017-sentence}{{68}{}{{}}{{}}}
\bibcite{snell2019word}{{69}{}{{}}{{}}}
\bibcite{sst2_socher2013recursive}{{70}{}{{}}{{}}}
\bibcite{2018arXiv181107017S}{{71}{}{{}}{{}}}
\bibcite{stanojevic2021}{{72}{}{{}}{{}}}
\bibcite{strubell2018}{{73}{}{{}}{{}}}
\bibcite{toyota-2001-changes}{{74}{}{{}}{{}}}
\bibcite{vaswani2017attention}{{75}{}{{}}{{}}}
\bibcite{vaswani-etal-2017-attention}{{76}{}{{}}{{}}}
\bibcite{Velickovic2017-mh}{{77}{}{{}}{{}}}
\bibcite{wang-etal-2019-superglue}{{78}{}{{}}{{}}}
\bibcite{wang-etal-2018-glue}{{79}{}{{}}{{}}}
\bibcite{wang2018glue}{{80}{}{{}}{{}}}
\bibcite{warstadt-bowman-2020-can}{{81}{}{{}}{{}}}
\bibcite{cola_warstadt2019neural}{{82}{}{{}}{{}}}
\bibcite{wen-etal-2019-parallel}{{83}{}{{}}{{}}}
\bibcite{williams-etal-2018-broad}{{84}{}{{}}{{}}}
\bibcite{wolf2019}{{85}{}{{}}{{}}}
\bibcite{wu-etal-2020-perturbed}{{86}{}{{}}{{}}}
\bibcite{zhang2019paws}{{87}{}{{}}{{}}}
\bibcite{zhu2015aligning}{{88}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
