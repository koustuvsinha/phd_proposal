\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{abbrv}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani-etal-2017-attention}
\citation{devlin2018bert}
\citation{devlin2018bert,liu-et-al-2019-roberta,lewis-etal-2020-bart}
\citation{rogers2020}
\citation{tenney-etal-2019-bert}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert}
\citation{ettinger2020}
\citation{petroni2019language,rogers2020}
\citation{gentner1986systematicity}
\citation{fodor1988connectionism}
\citation{jia2016,jin2020bert}
\citation{gururangan2018a,poliak-etal-2018-hypothesis,tsuchiya-2018-performance,naik-etal-2018-stress,mccoy2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{sinha2019a}
\citation{goodwin2020}
\citation{gontier2020}
\citation{sinha2021}
\citation{sinha2021a}
\citation{parthasarathi2021a}
\citation{sinha2020c}
\citation{sinha2020d}
\citation{bordes2013translating}
\citation{bengio2013representation}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background: Natural language understanding through Neural approaches}{2}{section.2}\protected@file@percent }
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{kim2014convolutional}
\citation{vaswani-etal-2017-attention}
\citation{radford2018improving}
\citation{raffel2019exploring}
\citation{zhu2015aligning}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{peters2018deep}
\citation{howard2018universal}
\citation{devlin2018bert}
\citation{qiu2020}
\citation{rogers2020}
\citation{hupkes2018visualisation}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert}
\citation{tenney-etal-2019-bert,ettinger2020}
\citation{petroni2019language,rogers2020}
\citation{jia2016,jin2020bert}
\citation{kaushik2018much}
\citation{gururangan2018a,poliak-etal-2018-hypothesis,tsuchiya-2018-performance}
\citation{naik-etal-2018-stress,mccoy2019}
\citation{lake2017generalization}
\citation{dasgupta-etal-2018-evaluating}
\citation{sinha2019a}
\citation{hinton1986learning,muggleton1991inductive}
\@writefile{toc}{\contentsline {section}{\numberline {3}Contribution 1: Investigating systematicity of NLU models using first order logic}{4}{section.3}\protected@file@percent }
\newlabel{sec:cont1}{{3}{4}{Contribution 1: Investigating systematicity of NLU models using first order logic}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces CLUTRR inductive reasoning task\relax }}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:clutrr_data}{{1}{4}{CLUTRR inductive reasoning task\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CLUTRR dataset design steps\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:clutrr_data_design}{{2}{4}{CLUTRR dataset design steps\relax }{figure.caption.2}{}}
\citation{hochreiter1997long,cho2014learning}
\citation{santoro2017simple}
\citation{hudson2018compositional}
\citation{devlin2018bert}
\citation{Velickovic2017-mh}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dataset}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments}{5}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Systematic generalization results on CLUTRR, when trained on stories of length $k=2,3,4$\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:clutrr_sys_gen_234}{{3}{5}{Systematic generalization results on CLUTRR, when trained on stories of length $k=2,3,4$\relax }{figure.caption.3}{}}
\citation{lake2017generalization,2018arXiv181107017S}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Testing the robustness of the various models when training and testing on stories containing various types of noise facts.\relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{tab:robust}{{1}{6}{Testing the robustness of the various models when training and testing on stories containing various types of noise facts.\relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Noise generation methods in CLUTRR\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:clutrr_data_noise}{{4}{6}{Noise generation methods in CLUTRR\relax }{figure.caption.4}{}}
\citation{goodwin2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{7}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Follow-up Works}{7}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces A template for sentences in the artificial language. Each sentence fills the obligatory positions 1, 3, and 6 with a word: a quantifier, noun, and verb. Optional positions (2, 4 and 5) are filled by either a word (adjective, postmodifier or negation) or by the empty string. Closed-class categories (Quantifiers, adjectives, post modifiers and negation) do not include novel words, while open-class categories (nouns and verbs) includes novel words that are only exposed in the test set. \relax }}{7}{table.caption.6}\protected@file@percent }
\newlabel{tbl:artlang}{{2}{7}{A template for sentences in the artificial language. Each sentence fills the obligatory positions 1, 3, and 6 with a word: a quantifier, noun, and verb. Optional positions (2, 4 and 5) are filled by either a word (adjective, postmodifier or negation) or by the empty string. Closed-class categories (Quantifiers, adjectives, post modifiers and negation) do not include novel words, while open-class categories (nouns and verbs) includes novel words that are only exposed in the test set. \relax }{table.caption.6}{}}
\citation{gontier2020}
\citation{Evans2017-pu}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Systematic generalization issues in proof generation\relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:proof_sys_gen}{{5}{8}{Systematic generalization issues in proof generation\relax }{figure.caption.7}{}}
\citation{vaswani-etal-2017-attention}
\citation{liu-et-al-2019-roberta}
\citation{lewis-etal-2020-bart}
\citation{radford-etal-2019-language,brown-etal-2020-gpt3}
\citation{wang-etal-2018-glue,wang-etal-2019-superglue}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert,warstadt-bowman-2020-can,wu-etal-2020-perturbed}
\citation{rogers2020}
\citation{sinha2021,sinha2021a}
\citation{sinha2021}
\citation{chomsky-1995-minimalist}
\citation{harris-1954-distributional}
\citation{cattell-1886-time,scheerer1981early,toyota-2001-changes,baddeley-etal-2009-working,snell-grainger-2017-sentence,snell2019word,wen-etal-2019-parallel}
\citation{heim-kratzer-1998-semantics}
\citation{bender-koller-2020-climbing}
\@writefile{toc}{\contentsline {section}{\numberline {4}Contribution 2: Probing systematicity of pre-trained models using word order}{9}{section.4}\protected@file@percent }
\newlabel{sec:cont2}{{4}{9}{Contribution 2: Probing systematicity of pre-trained models using word order}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}UnNatural Language Inference}{9}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Motivation}{9}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Examples from the MNLI Matched development set. Both the original example and the permuted one elicit the same classification label (entailment and contradiction respectively) from RoBERTa (large).\relax }}{9}{table.caption.8}\protected@file@percent }
\newlabel{tab:example}{{3}{9}{Examples from the MNLI Matched development set. Both the original example and the permuted one elicit the same classification label (entailment and contradiction respectively) from RoBERTa (large).\relax }{table.caption.8}{}}
\citation{condoravdi-etal-2003-entailment,dagan-etal-2005-pascal}
\citation{bowman-etal-2015-large}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{nie-etal-2020-adversarial}
\citation{hu-etal-2020-ocnli}
\citation{sinha2021a}
\citation{hu-etal-2020-ocnli}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Experiments and Results}{10}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model confidences on unnatural input\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:all_entropy}{{6}{10}{Model confidences on unnatural input\relax }{figure.caption.9}{}}
\citation{mollica-2020-composition}
\citation{gupta-etal-2021-bert}
\citation{dasgupta-etal-2018-evaluating,poliak-etal-2018-hypothesis,gururangan-etal-2018-annotation,naik-etal-2019-exploring}
\citation{sinha2021}
\citation{sinha2021a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Discussion}{11}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Masked Language Modeling and the Distributional Hypothesis: \textit  {Order word matters pre-training for little}}{11}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{11}{subsubsection.4.2.1}\protected@file@percent }
\citation{liu2019b}
\citation{zhu2015aligning}
\citation{liu2019b}
\citation{sinha2021a}
\citation{wang2018glue}
\citation{cola_warstadt2019neural}
\citation{sst2_socher2013recursive}
\citation{mrpc_dolan2005automatically}
\citation{williams-etal-2018-broad}
\citation{qnli_rajpurkar2016squad,qnli_2_demszky2018transforming}
\citation{rte1_dagan2005pascal,rte2_haim2006second,rte3_giampiccolo2007third,rte5_bentivogli2009fifth}
\citation{zhang2019paws}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Experiments \& Results}{12}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Fine-tuning results on various word shuffled pre-trained models.\relax }}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:masked_results}{{7}{12}{Fine-tuning results on various word shuffled pre-trained models.\relax }{figure.caption.10}{}}
\citation{pham2020}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces GLUE \& PAWS task dev performance when finetuned on naturally and randomly ordered text, respectively, using pre-trained RoBERTa (base) models trained on different versions of BookWiki corpus.\relax }}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig:word_order_glue}{{8}{13}{GLUE \& PAWS task dev performance when finetuned on naturally and randomly ordered text, respectively, using pre-trained RoBERTa (base) models trained on different versions of BookWiki corpus.\relax }{figure.caption.11}{}}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{conneau-etal-2018-cram}
\citation{conneau-kiela-2018-senteval}
\citation{jawahar2019a}
\citation{linzen-etal-2016-assessing}
\citation{gulordava2018}
\citation{marvin-linzen-2018-targeted}
\citation{linzen-etal-2016-assessing}
\citation{gulordava2018}
\citation{marvin-linzen-2018-targeted}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted,gulordava2018,goldberga,wolf2019}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted}
\citation{chomsky1957syntactic}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Unlabeled Attachment Score (UAS) (mean and std) on the dependency parsing task (DEP) on two datasets, UD EWT and PTB, using the Pareto Probing framework \cite  {pimentel2020b}.\relax }}{14}{table.caption.12}\protected@file@percent }
\newlabel{tab:pareto_dependency}{{4}{14}{Unlabeled Attachment Score (UAS) (mean and std) on the dependency parsing task (DEP) on two datasets, UD EWT and PTB, using the Pareto Probing framework \cite {pimentel2020b}.\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Mean (and std) non-parametric probing accuracy on different datasets - Linzen et al. (LZ) \cite  {linzen-etal-2016-assessing}, Gulordava et al. (GL) \cite  {gulordava2018} and Marvin et al (MA) \cite  {marvin-linzen-2018-targeted}\relax }}{14}{table.caption.13}\protected@file@percent }
\newlabel{tab:non_param_comb}{{5}{14}{Mean (and std) non-parametric probing accuracy on different datasets - Linzen et al. (LZ) \cite {linzen-etal-2016-assessing}, Gulordava et al. (GL) \cite {gulordava2018} and Marvin et al (MA) \cite {marvin-linzen-2018-targeted}\relax }{table.caption.13}{}}
\citation{fundel2007relex}
\citation{jie2019dependency}
\citation{strubell2018}
\citation{sachan2021}
\citation{stanojevic2021}
\citation{nie2020}
\citation{gururangan2018a,poliak-etal-2018-hypothesis,tsuchiya-2018-performance}
\citation{maudslay2020b}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discussion \& Conclusion}{15}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Future Work \& Timeline}{15}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Unsupervised syntax learning by mutually exclusive training using word order}{15}{subsection.5.1}\protected@file@percent }
\citation{mccoy2019}
\bibdata{main}
\bibcite{baddeley-etal-2009-working}{{1}{}{{}}{{}}}
\bibcite{bender-koller-2020-climbing}{{2}{}{{}}{{}}}
\bibcite{bengio2013representation}{{3}{}{{}}{{}}}
\bibcite{rte5_bentivogli2009fifth}{{4}{}{{}}{{}}}
\bibcite{bordes2013translating}{{5}{}{{}}{{}}}
\bibcite{bowman-etal-2015-large}{{6}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Nonsensical data augmentation for better systematic generalization}{16}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Timeline}{16}{subsection.5.3}\protected@file@percent }
\bibcite{brown-etal-2020-gpt3}{{7}{}{{}}{{}}}
\bibcite{cattell-1886-time}{{8}{}{{}}{{}}}
\bibcite{cho2014learning}{{9}{}{{}}{{}}}
\bibcite{chomsky1957syntactic}{{10}{}{{}}{{}}}
\bibcite{chomsky-1995-minimalist}{{11}{}{{}}{{}}}
\bibcite{collobert2008unified}{{12}{}{{}}{{}}}
\bibcite{condoravdi-etal-2003-entailment}{{13}{}{{}}{{}}}
\bibcite{conneau-kiela-2018-senteval}{{14}{}{{}}{{}}}
\bibcite{conneau-etal-2018-cram}{{15}{}{{}}{{}}}
\bibcite{dagan-etal-2005-pascal}{{16}{}{{}}{{}}}
\bibcite{rte1_dagan2005pascal}{{17}{}{{}}{{}}}
\bibcite{dasgupta-etal-2018-evaluating}{{18}{}{{}}{{}}}
\bibcite{qnli_2_demszky2018transforming}{{19}{}{{}}{{}}}
\bibcite{devlin2018bert}{{20}{}{{}}{{}}}
\bibcite{mrpc_dolan2005automatically}{{21}{}{{}}{{}}}
\bibcite{ettinger2020}{{22}{}{{}}{{}}}
\bibcite{Evans2017-pu}{{23}{}{{}}{{}}}
\bibcite{fodor1988connectionism}{{24}{}{{}}{{}}}
\bibcite{fundel2007relex}{{25}{}{{}}{{}}}
\bibcite{gentner1986systematicity}{{26}{}{{}}{{}}}
\bibcite{rte3_giampiccolo2007third}{{27}{}{{}}{{}}}
\bibcite{goldberga}{{28}{}{{}}{{}}}
\bibcite{gontier2020}{{29}{}{{}}{{}}}
\bibcite{goodwin2020}{{30}{}{{}}{{}}}
\bibcite{gulordava2018}{{31}{}{{}}{{}}}
\bibcite{gupta-etal-2021-bert}{{32}{}{{}}{{}}}
\bibcite{gururangan-etal-2018-annotation}{{33}{}{{}}{{}}}
\bibcite{gururangan2018a}{{34}{}{{}}{{}}}
\bibcite{rte2_haim2006second}{{35}{}{{}}{{}}}
\bibcite{harris-1954-distributional}{{36}{}{{}}{{}}}
\bibcite{heim-kratzer-1998-semantics}{{37}{}{{}}{{}}}
\bibcite{hewitt-manning-2019-structural}{{38}{}{{}}{{}}}
\bibcite{hinton1986learning}{{39}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{40}{}{{}}{{}}}
\bibcite{howard2018universal}{{41}{}{{}}{{}}}
\bibcite{hu-etal-2020-ocnli}{{42}{}{{}}{{}}}
\bibcite{hudson2018compositional}{{43}{}{{}}{{}}}
\bibcite{hupkes2018visualisation}{{44}{}{{}}{{}}}
\bibcite{jawahar-etal-2019-bert}{{45}{}{{}}{{}}}
\bibcite{jawahar2019a}{{46}{}{{}}{{}}}
\bibcite{jia2016}{{47}{}{{}}{{}}}
\bibcite{jie2019dependency}{{48}{}{{}}{{}}}
\bibcite{jin2020bert}{{49}{}{{}}{{}}}
\bibcite{kaushik2018much}{{50}{}{{}}{{}}}
\bibcite{kim2014convolutional}{{51}{}{{}}{{}}}
\bibcite{lake2017generalization}{{52}{}{{}}{{}}}
\bibcite{lewis-etal-2020-bart}{{53}{}{{}}{{}}}
\bibcite{linzen-etal-2016-assessing}{{54}{}{{}}{{}}}
\bibcite{liu-et-al-2019-roberta}{{55}{}{{}}{{}}}
\bibcite{liu2019b}{{56}{}{{}}{{}}}
\bibcite{marvin-linzen-2018-targeted}{{57}{}{{}}{{}}}
\bibcite{maudslay2020b}{{58}{}{{}}{{}}}
\bibcite{mccoy2019}{{59}{}{{}}{{}}}
\bibcite{mikolov2013efficient}{{60}{}{{}}{{}}}
\bibcite{mollica-2020-composition}{{61}{}{{}}{{}}}
\bibcite{muggleton1991inductive}{{62}{}{{}}{{}}}
\bibcite{naik-etal-2019-exploring}{{63}{}{{}}{{}}}
\bibcite{naik-etal-2018-stress}{{64}{}{{}}{{}}}
\bibcite{nie-etal-2020-adversarial}{{65}{}{{}}{{}}}
\bibcite{nie2020}{{66}{}{{}}{{}}}
\bibcite{parthasarathi2021a}{{67}{}{{}}{{}}}
\bibcite{pennington2014glove}{{68}{}{{}}{{}}}
\bibcite{peters2018deep}{{69}{}{{}}{{}}}
\bibcite{petroni2019language}{{70}{}{{}}{{}}}
\bibcite{pham2020}{{71}{}{{}}{{}}}
\bibcite{pimentel2020b}{{72}{}{{}}{{}}}
\bibcite{poliak-etal-2018-hypothesis}{{73}{}{{}}{{}}}
\bibcite{qiu2020}{{74}{}{{}}{{}}}
\bibcite{radford2018improving}{{75}{}{{}}{{}}}
\bibcite{radford-etal-2019-language}{{76}{}{{}}{{}}}
\bibcite{raffel2019exploring}{{77}{}{{}}{{}}}
\bibcite{qnli_rajpurkar2016squad}{{78}{}{{}}{{}}}
\bibcite{rogers2020}{{79}{}{{}}{{}}}
\bibcite{sachan2021}{{80}{}{{}}{{}}}
\bibcite{santoro2017simple}{{81}{}{{}}{{}}}
\bibcite{scheerer1981early}{{82}{}{{}}{{}}}
\bibcite{sinha2021a}{{83}{}{{}}{{}}}
\bibcite{sinha2021}{{84}{}{{}}{{}}}
\bibcite{sinha2020d}{{85}{}{{}}{{}}}
\bibcite{sinha2019a}{{86}{}{{}}{{}}}
\bibcite{sinha2020c}{{87}{}{{}}{{}}}
\bibcite{snell-grainger-2017-sentence}{{88}{}{{}}{{}}}
\bibcite{snell2019word}{{89}{}{{}}{{}}}
\bibcite{sst2_socher2013recursive}{{90}{}{{}}{{}}}
\bibcite{2018arXiv181107017S}{{91}{}{{}}{{}}}
\bibcite{stanojevic2021}{{92}{}{{}}{{}}}
\bibcite{strubell2018}{{93}{}{{}}{{}}}
\bibcite{tenney-etal-2019-bert}{{94}{}{{}}{{}}}
\bibcite{toyota-2001-changes}{{95}{}{{}}{{}}}
\bibcite{tsuchiya-2018-performance}{{96}{}{{}}{{}}}
\bibcite{vaswani2017attention}{{97}{}{{}}{{}}}
\bibcite{vaswani-etal-2017-attention}{{98}{}{{}}{{}}}
\bibcite{Velickovic2017-mh}{{99}{}{{}}{{}}}
\bibcite{wang-etal-2019-superglue}{{100}{}{{}}{{}}}
\bibcite{wang-etal-2018-glue}{{101}{}{{}}{{}}}
\bibcite{wang2018glue}{{102}{}{{}}{{}}}
\bibcite{warstadt-bowman-2020-can}{{103}{}{{}}{{}}}
\bibcite{cola_warstadt2019neural}{{104}{}{{}}{{}}}
\bibcite{wen-etal-2019-parallel}{{105}{}{{}}{{}}}
\bibcite{williams-etal-2018-broad}{{106}{}{{}}{{}}}
\bibcite{wolf2019}{{107}{}{{}}{{}}}
\bibcite{wu-etal-2020-perturbed}{{108}{}{{}}{{}}}
\bibcite{zhang2019paws}{{109}{}{{}}{{}}}
\bibcite{zhu2015aligning}{{110}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
