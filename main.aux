\relax 
\bibstyle{abbrv}
\citation{vaswani-etal-2017-attention}
\citation{devlin2018bert}
\citation{devlin2018bert,liu-et-al-2019-roberta,lewis-etal-2020-bart}
\citation{rogers2020}
\citation{tenney-etal-2019-bert}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert}
\citation{ettinger2020}
\citation{petroni2019language,rogers2020}
\citation{fodor1988connectionism}
\citation{jia2016,jin2020bert}
\citation{gururangan2018a,poliak-etal-2018-hypothesis,tsuchiya-2018-performance,naik-etal-2018-stress,mccoy2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{sinha2019a}
\citation{goodwin2020}
\citation{gontier2020}
\citation{sinha2021}
\citation{sinha2021a}
\citation{parthasarathi2021a}
\citation{sinha2020c}
\citation{sinha2020d}
\citation{bordes2013translating}
\citation{bengio2013representation}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{kim2014convolutional}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background: Natural language understanding through Neural approaches}{2}\protected@file@percent }
\citation{vaswani-etal-2017-attention}
\citation{radford2018improving}
\citation{raffel2019exploring}
\citation{zhu2015aligning}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{peters2018deep}
\citation{howard2018universal}
\citation{devlin2018bert}
\citation{qiu2020}
\citation{rogers2020}
\citation{hupkes2018visualisation}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert}
\citation{tenney-etal-2019-bert,ettinger2020}
\citation{petroni2019language,rogers2020}
\citation{jia2016,jin2020bert}
\citation{kaushik2018much}
\citation{gururangan2018a,poliak-etal-2018-hypothesis,tsuchiya-2018-performance}
\citation{naik-etal-2018-stress,mccoy2019}
\citation{lake2017generalization}
\citation{dasgupta-etal-2018-evaluating}
\citation{hinton1986learning,muggleton1991inductive}
\@writefile{toc}{\contentsline {section}{\numberline {3}Contribution 1: Investigating systematicity of NLU models using first order logic}{4}\protected@file@percent }
\newlabel{sec:cont1}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces CLUTRR inductive reasoning task\relax }}{4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:clutrr_data}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CLUTRR dataset design steps\relax }}{4}\protected@file@percent }
\newlabel{fig:clutrr_data_design}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dataset}{4}\protected@file@percent }
\citation{hochreiter1997long,cho2014learning}
\citation{santoro2017simple}
\citation{hudson2018compositional}
\citation{devlin2018bert}
\citation{Velickovic2017-mh}
\citation{lake2017generalization,2018arXiv181107017S}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Systematic generalization results on CLUTRR, when trained on stories of length $k=2,3,4$\relax }}{5}\protected@file@percent }
\newlabel{fig:clutrr_sys_gen_234}{{3}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Testing the robustness of the various models when training and testing on stories containing various types of noise facts.\relax }}{6}\protected@file@percent }
\newlabel{tab:robust}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Noise generation methods in CLUTRR\relax }}{6}\protected@file@percent }
\newlabel{fig:clutrr_data_noise}{{4}{6}}
\citation{goodwin2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Related Works}{7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces A template for sentences in the artificial language. Each sentence fills the obligatory positions 1, 3, and 6 with a word: a quantifier, noun, and verb. Optional positions (2, 4 and 5) are filled by either a word (adjective, postmodifier or negation) or by the empty string. Closed-class categories (Quantifiers, adjectives, post modifiers and negation) do not include novel words, while open-class categories (nouns and verbs) includes novel words that are only exposed in the test set. \relax }}{7}\protected@file@percent }
\newlabel{tbl:artlang}{{2}{7}}
\citation{gontier2020}
\citation{Evans2017-pu}
\citation{vaswani2017attention}
\citation{vaswani-etal-2017-attention}
\citation{liu-et-al-2019-roberta}
\citation{lewis-etal-2020-bart}
\citation{radford-etal-2019-language,brown-etal-2020-gpt3}
\citation{wang-etal-2018-glue,wang-etal-2019-superglue}
\citation{hewitt-manning-2019-structural,jawahar-etal-2019-bert,warstadt-bowman-2020-can,wu-etal-2020-perturbed}
\citation{rogers2020}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Systematic generalization issues in proof generation\relax }}{8}\protected@file@percent }
\newlabel{fig:proof_sys_gen}{{5}{8}}
\citation{sinha2021,sinha2021a}
\citation{sinha2021}
\citation{chomsky-1995-minimalist}
\citation{harris-1954-distributional}
\citation{cattell-1886-time,scheerer1981early,toyota-2001-changes,baddeley-etal-2009-working,snell-grainger-2017-sentence,snell2019word,wen-etal-2019-parallel}
\citation{heim-kratzer-1998-semantics}
\citation{bender-koller-2020-climbing}
\citation{condoravdi-etal-2003-entailment,dagan-etal-2005-pascal}
\citation{bowman-etal-2015-large}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{nie-etal-2020-adversarial}
\citation{hu-etal-2020-ocnli}
\@writefile{toc}{\contentsline {section}{\numberline {4}Contribution 2: Probing systematicity of pre-trained models using word order}{9}\protected@file@percent }
\newlabel{sec:cont2}{{4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}UnNatural Language Inference}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Motivation}{9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Examples from the MNLI Matched development set. Both the original example and the permuted one elicit the same classification label (entailment and contradiction respectively) from RoBERTa (large).\relax }}{9}\protected@file@percent }
\newlabel{tab:example}{{3}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Experiments and Results}{9}\protected@file@percent }
\citation{sinha2021a}
\citation{hu-etal-2020-ocnli}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model confidences on unnatural input\relax }}{10}\protected@file@percent }
\newlabel{fig:all_entropy}{{6}{10}}
\citation{mollica-2020-composition}
\citation{gupta-etal-2021-bert}
\citation{dasgupta-etal-2018-evaluating,poliak-etal-2018-hypothesis,gururangan-etal-2018-annotation,naik-etal-2019-exploring}
\citation{sinha2021}
\citation{sinha2021a}
\citation{liu2019b}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Discussion}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Masked Language Modeling and the Distributional Hypothesis: \textit  {Order word matters pre-training for little}}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{11}\protected@file@percent }
\citation{zhu2015aligning}
\citation{liu2019b}
\citation{sinha2021a}
\citation{wang2018glue}
\citation{cola_warstadt2019neural}
\citation{sst2_socher2013recursive}
\citation{mrpc_dolan2005automatically}
\citation{williams-etal-2018-broad}
\citation{qnli_rajpurkar2016squad,qnli_2_demszky2018transforming}
\citation{rte1_dagan2005pascal,rte2_haim2006second,rte3_giampiccolo2007third,rte5_bentivogli2009fifth}
\citation{zhang2019paws}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Experiments \& Results}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Fine-tuning results on various word shuffled pre-trained models.\relax }}{12}\protected@file@percent }
\newlabel{fig:masked_results}{{7}{12}}
\citation{pham2020}
\citation{pimentel2020b}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces GLUE \& PAWS task dev performance when finetuned on naturally and randomly ordered text, respectively, using pre-trained RoBERTa (base) models trained on different versions of BookWiki corpus.\relax }}{13}\protected@file@percent }
\newlabel{fig:word_order_glue}{{8}{13}}
\citation{pimentel2020b}
\citation{pimentel2020b}
\citation{conneau-etal-2018-cram}
\citation{conneau-kiela-2018-senteval}
\citation{jawahar2019a}
\citation{linzen-etal-2016-assessing}
\citation{gulordava2018}
\citation{marvin-linzen-2018-targeted}
\citation{linzen-etal-2016-assessing}
\citation{gulordava2018}
\citation{marvin-linzen-2018-targeted}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted,gulordava2018,goldberga,wolf2019}
\citation{linzen-etal-2016-assessing,marvin-linzen-2018-targeted}
\citation{chomsky1957syntactic}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Unlabeled Attachment Score (UAS) (mean and std) on the dependency parsing task (DEP) on two datasets, UD EWT and PTB, using the Pareto Probing framework \cite  {pimentel2020b}.\relax }}{14}\protected@file@percent }
\newlabel{tab:pareto_dependency}{{4}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Mean (and std) non-parametric probing accuracy on different datasets - Linzen et al. (LZ) \cite  {linzen-etal-2016-assessing}, Gulordava et al. (GL) \cite  {gulordava2018} and Marvin et al (MA) \cite  {marvin-linzen-2018-targeted}\relax }}{14}\protected@file@percent }
\newlabel{tab:non_param_comb}{{5}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discussion \& Conclusion}{14}\protected@file@percent }
\citation{fundel2007relex}
\citation{jie2019dependency}
\citation{strubell2018}
\citation{sachan2021}
\citation{stanojevic2021}
\citation{nie2020}
\citation{gururangan2018a,poliak-etal-2018-hypothesis,tsuchiya-2018-performance}
\citation{maudslay2020b}
\@writefile{toc}{\contentsline {section}{\numberline {5}Future Work \& Timeline}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Unsupervised syntax learning by mutually exclusive training using word order}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Nonsensical data augmentation for better systematic generalization}{15}\protected@file@percent }
\citation{mccoy2019}
\bibdata{main}
\bibcite{baddeley-etal-2009-working}{{1}{}{{}}{{}}}
\bibcite{bender-koller-2020-climbing}{{2}{}{{}}{{}}}
\bibcite{bengio2013representation}{{3}{}{{}}{{}}}
\bibcite{rte5_bentivogli2009fifth}{{4}{}{{}}{{}}}
\bibcite{bordes2013translating}{{5}{}{{}}{{}}}
\bibcite{bowman-etal-2015-large}{{6}{}{{}}{{}}}
\bibcite{brown-etal-2020-gpt3}{{7}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Timeline}{16}\protected@file@percent }
\bibcite{cattell-1886-time}{{8}{}{{}}{{}}}
\bibcite{cho2014learning}{{9}{}{{}}{{}}}
\bibcite{chomsky1957syntactic}{{10}{}{{}}{{}}}
\bibcite{chomsky-1995-minimalist}{{11}{}{{}}{{}}}
\bibcite{collobert2008unified}{{12}{}{{}}{{}}}
\bibcite{condoravdi-etal-2003-entailment}{{13}{}{{}}{{}}}
\bibcite{conneau-kiela-2018-senteval}{{14}{}{{}}{{}}}
\bibcite{conneau-etal-2018-cram}{{15}{}{{}}{{}}}
\bibcite{dagan-etal-2005-pascal}{{16}{}{{}}{{}}}
\bibcite{rte1_dagan2005pascal}{{17}{}{{}}{{}}}
\bibcite{dasgupta-etal-2018-evaluating}{{18}{}{{}}{{}}}
\bibcite{qnli_2_demszky2018transforming}{{19}{}{{}}{{}}}
\bibcite{devlin2018bert}{{20}{}{{}}{{}}}
\bibcite{mrpc_dolan2005automatically}{{21}{}{{}}{{}}}
\bibcite{ettinger2020}{{22}{}{{}}{{}}}
\bibcite{Evans2017-pu}{{23}{}{{}}{{}}}
\bibcite{fodor1988connectionism}{{24}{}{{}}{{}}}
\bibcite{fundel2007relex}{{25}{}{{}}{{}}}
\bibcite{rte3_giampiccolo2007third}{{26}{}{{}}{{}}}
\bibcite{goldberga}{{27}{}{{}}{{}}}
\bibcite{gontier2020}{{28}{}{{}}{{}}}
\bibcite{goodwin2020}{{29}{}{{}}{{}}}
\bibcite{gulordava2018}{{30}{}{{}}{{}}}
\bibcite{gupta-etal-2021-bert}{{31}{}{{}}{{}}}
\bibcite{gururangan-etal-2018-annotation}{{32}{}{{}}{{}}}
\bibcite{gururangan2018a}{{33}{}{{}}{{}}}
\bibcite{rte2_haim2006second}{{34}{}{{}}{{}}}
\bibcite{harris-1954-distributional}{{35}{}{{}}{{}}}
\bibcite{heim-kratzer-1998-semantics}{{36}{}{{}}{{}}}
\bibcite{hewitt-manning-2019-structural}{{37}{}{{}}{{}}}
\bibcite{hinton1986learning}{{38}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{39}{}{{}}{{}}}
\bibcite{howard2018universal}{{40}{}{{}}{{}}}
\bibcite{hu-etal-2020-ocnli}{{41}{}{{}}{{}}}
\bibcite{hudson2018compositional}{{42}{}{{}}{{}}}
\bibcite{hupkes2018visualisation}{{43}{}{{}}{{}}}
\bibcite{jawahar-etal-2019-bert}{{44}{}{{}}{{}}}
\bibcite{jawahar2019a}{{45}{}{{}}{{}}}
\bibcite{jia2016}{{46}{}{{}}{{}}}
\bibcite{jie2019dependency}{{47}{}{{}}{{}}}
\bibcite{jin2020bert}{{48}{}{{}}{{}}}
\bibcite{kaushik2018much}{{49}{}{{}}{{}}}
\bibcite{kim2014convolutional}{{50}{}{{}}{{}}}
\bibcite{lake2017generalization}{{51}{}{{}}{{}}}
\bibcite{lewis-etal-2020-bart}{{52}{}{{}}{{}}}
\bibcite{linzen-etal-2016-assessing}{{53}{}{{}}{{}}}
\bibcite{liu-et-al-2019-roberta}{{54}{}{{}}{{}}}
\bibcite{liu2019b}{{55}{}{{}}{{}}}
\bibcite{marvin-linzen-2018-targeted}{{56}{}{{}}{{}}}
\bibcite{maudslay2020b}{{57}{}{{}}{{}}}
\bibcite{mccoy2019}{{58}{}{{}}{{}}}
\bibcite{mikolov2013efficient}{{59}{}{{}}{{}}}
\bibcite{mollica-2020-composition}{{60}{}{{}}{{}}}
\bibcite{muggleton1991inductive}{{61}{}{{}}{{}}}
\bibcite{naik-etal-2019-exploring}{{62}{}{{}}{{}}}
\bibcite{naik-etal-2018-stress}{{63}{}{{}}{{}}}
\bibcite{nie-etal-2020-adversarial}{{64}{}{{}}{{}}}
\bibcite{nie2020}{{65}{}{{}}{{}}}
\bibcite{parthasarathi2021a}{{66}{}{{}}{{}}}
\bibcite{pennington2014glove}{{67}{}{{}}{{}}}
\bibcite{peters2018deep}{{68}{}{{}}{{}}}
\bibcite{petroni2019language}{{69}{}{{}}{{}}}
\bibcite{pham2020}{{70}{}{{}}{{}}}
\bibcite{pimentel2020b}{{71}{}{{}}{{}}}
\bibcite{poliak-etal-2018-hypothesis}{{72}{}{{}}{{}}}
\bibcite{qiu2020}{{73}{}{{}}{{}}}
\bibcite{radford2018improving}{{74}{}{{}}{{}}}
\bibcite{radford-etal-2019-language}{{75}{}{{}}{{}}}
\bibcite{raffel2019exploring}{{76}{}{{}}{{}}}
\bibcite{qnli_rajpurkar2016squad}{{77}{}{{}}{{}}}
\bibcite{rogers2020}{{78}{}{{}}{{}}}
\bibcite{sachan2021}{{79}{}{{}}{{}}}
\bibcite{santoro2017simple}{{80}{}{{}}{{}}}
\bibcite{scheerer1981early}{{81}{}{{}}{{}}}
\bibcite{sinha2021a}{{82}{}{{}}{{}}}
\bibcite{sinha2021}{{83}{}{{}}{{}}}
\bibcite{sinha2020d}{{84}{}{{}}{{}}}
\bibcite{sinha2019a}{{85}{}{{}}{{}}}
\bibcite{sinha2020c}{{86}{}{{}}{{}}}
\bibcite{snell-grainger-2017-sentence}{{87}{}{{}}{{}}}
\bibcite{snell2019word}{{88}{}{{}}{{}}}
\bibcite{sst2_socher2013recursive}{{89}{}{{}}{{}}}
\bibcite{2018arXiv181107017S}{{90}{}{{}}{{}}}
\bibcite{stanojevic2021}{{91}{}{{}}{{}}}
\bibcite{strubell2018}{{92}{}{{}}{{}}}
\bibcite{tenney-etal-2019-bert}{{93}{}{{}}{{}}}
\bibcite{toyota-2001-changes}{{94}{}{{}}{{}}}
\bibcite{tsuchiya-2018-performance}{{95}{}{{}}{{}}}
\bibcite{vaswani2017attention}{{96}{}{{}}{{}}}
\bibcite{vaswani-etal-2017-attention}{{97}{}{{}}{{}}}
\bibcite{Velickovic2017-mh}{{98}{}{{}}{{}}}
\bibcite{wang-etal-2019-superglue}{{99}{}{{}}{{}}}
\bibcite{wang-etal-2018-glue}{{100}{}{{}}{{}}}
\bibcite{wang2018glue}{{101}{}{{}}{{}}}
\bibcite{warstadt-bowman-2020-can}{{102}{}{{}}{{}}}
\bibcite{cola_warstadt2019neural}{{103}{}{{}}{{}}}
\bibcite{wen-etal-2019-parallel}{{104}{}{{}}{{}}}
\bibcite{williams-etal-2018-broad}{{105}{}{{}}{{}}}
\bibcite{wolf2019}{{106}{}{{}}{{}}}
\bibcite{wu-etal-2020-perturbed}{{107}{}{{}}{{}}}
\bibcite{zhang2019paws}{{108}{}{{}}{{}}}
\bibcite{zhu2015aligning}{{109}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
